## 今日学习
- pytorch
- 讲座
## pytorch
- 发现菜鸟教程完爆小土堆，小土堆纯纯垃圾
## 临时存储区域
Tanh
**Leaky ReLU、Parametric ReLU、ELU**
. 权重初始化策略

- **Xavier 初始化（Glorot 初始化）** ：适用于 Tanh 等对称激活函数；
- **He 初始化** ：适用于 ReLU 类激活函数；

3. 批归一化（Batch Normalization）

- 对每一层的输入进行标准化，使数据分布保持稳定；
- 缓解梯度消失/爆炸，加速训练。

> ✅ 在每一层卷积或全连接后加上 BatchNorm 层。

 4. 残差连接（Residual Connections）

- ResNet 中提出跳跃连接（skip connection），允许梯度直接传递到前面的层；
- 解决了深层网络中梯度消失的问题。

> ✅ 使用残差结构构建深层网络。

 5. 使用 LSTM / GRU（针对 RNN）

- 在循环神经网络中，LSTM 和 GRU 引入门控机制，缓解长期依赖问题；
- 允许梯度更长时间地流动。

> ✅ 在处理序列数据时优先考虑 LSTM 或 GRU。