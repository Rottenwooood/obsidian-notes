| 模型名称                   | 发布时间              | 是否使用合成数据训练 | 合成数据使用方式                                                                             | 真实演示依赖          | 备注与引用资料                                                                                                                                                                                                                                                                                                                                                                                          |
| ---------------------- | ----------------- | ---------- | ------------------------------------------------------------------------------------ | --------------- | ------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------ |
| **GraspVLA**           | 2025‑05           | ✅ 是        | SynGrasp‑1B：十亿帧 MuJoCo/Isaac Sim 仿真生成的抓取轨迹                                           | 少量真实用于微调        | 合成占主导，适配零样本泛化 ([xiaoxiao0406.github.io](https://xiaoxiao0406.github.io/vqvla.github.io/?utm_source=chatgpt.com "VQ-VLA: Improving Vision-Language-Action Models via ..."), [arXiv](https://arxiv.org/abs/2505.03233?utm_source=chatgpt.com "GraspVLA: a Grasping Foundation Model Pre-trained on Billion-scale Synthetic Action Data"))                                                          |
| **GR00T N1 / N1.5**    | 2025‑03 / 2025‑06 | ✅ 是        | Omniverse / Cosmos 合成轨迹 + 人类视频与真实机器人演示；N1.5 以 DreamGen 管道快速生成大量神经轨迹合成数据              | 极少真实示范          | 合成与真实混合推动泛化能力 ([phospho](https://blog.phospho.ai/dissecting-groot-n1-a-foundation-model-for-generalist-humanoid-robots/?utm_source=chatgpt.com "Dissecting GROOT N1: A Foundation Model for Generalist ..."), [phospho](https://blog.phospho.ai/d-groot-n1-5-a-foundation-model-for-generalist-humanoid-robots/?utm_source=chatgpt.com "Deep dive into GROOT N1.5: A Foundation Model for ...")) |
| **ReBot‑增强 OpenVLA**   | 2025‑03           | ✅ 是        | ReBot “real-to-sim-to-real” pipeline：真实轨迹 replay 仿真 + 合成视频训练增强                       | 基于真实轨迹增强        | 提升实际任务成功率 20%以上 ([phospho](https://blog.phospho.ai/dissecting-groot-n1-a-foundation-model-for-generalist-humanoid-robots/?utm_source=chatgpt.com "Dissecting GROOT N1: A Foundation Model for Generalist ..."))                                                                                                                                                                                  |
| **ReBot‑增强 Octo**      | 2025‑03           | ✅ 是        | 同上：将真实轨迹仿真 replay 并融合真实背景生成合成训练视频                                                    | 同上              | Octo 真实任 务性能提升约 17% ([phospho](https://blog.phospho.ai/dissecting-groot-n1-a-foundation-model-for-generalist-humanoid-robots/?utm_source=chatgpt.com "Dissecting GROOT N1: A Foundation Model for Generalist ..."))                                                                                                                                                                              |
| **Helix (Figure AI)**  | 2025‑02           | ✅ 是        | S1 系统使用约 ~500 小时仿真生成场景训练，结合域随机化等合成技术，大比例合成数据驱动训练                                     | 少量真实遥控演示        | 仿真占训练主导 ~78% ([维基百科](https://en.wikipedia.org/wiki/Vision-language-action_model?utm_source=chatgpt.com "Vision-language-action model"))                                                                                                                                                                                                                                                          |
| **Psi‑R1 (PsiBot)**    | 2025‑04           | ✅ 是        | 强化学习在仿真环境中执行麻将等长周期复杂任务，训练策略囊括 synthetic data                                         | 模拟为主            | 公司公开称“一切能力基于仿真 synthetic data” ([AIModels](https://www.aimodels.fyi/papers/arxiv/vq-vla-improving-vision-language-action-models?utm_source=chatgpt.com "VQ-VLA: Improving Vision-Language-Action Models via ..."))                                                                                                                                                                               |
| **Era‑42 (Robot Era)** | 2025‑中            | ✅ 是（有限披露）  | 官方称使用 DexVerse 平台生成混合物理仿真与 synthetic 操作流水进行训练                                        | 仿真为主            | 虽未详述轨迹数据细节，但明确使用 synthetic training ([Hugging Face](https://huggingface.co/papers?q=Vision-Language-Action+%28VLA%29+model&utm_source=chatgpt.com "Daily Papers"))                                                                                                                                                                                                                               |
| **VQ‑VLA**             | 2025‑07           | ✅ 是        | 训练 vector‑quantized action tokenizer 时利用大量**合成动作轨迹**，基于合成数据规模达前代 >100×，domain gap 较小 | 隶属 OpenVLA 微调步骤 | 隐含使用大规模合成轨迹训练 tokenizer，提升多任                                                                                                                                                                                                                                                                                                                                                                     |
