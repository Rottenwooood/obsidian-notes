### 001 先导课
	什么是机器学习
- 机器帮我们找一个复杂函数
	- 对语音识别
	- 对图像识别
	- 对alpha go

	`什么是深度学习
- 找一个类神经网络的复杂函数
	- 各式各样的输入：向量，矩阵（图像），序列（语音）
	- 各式各样的输出：
		- 数值scalar-回归任务regression
		- 类别-分类任务classification
		- 一段文字，一个图片
	- 1-5各个章节对应作业
		- 11，语音辨识简化12，影像辨识22，语音辨识32，机器翻译33

	`7什么是自监督学习`
- 数据无需标注
- 预训练生成Pre-trained Model，具备基础能力
	- BERT
	`6什么是生成式对抗网络`
- Generative Adversarial Networks
- 无需一对一的标注，只需一堆x，一堆y，自能找出对应关系
	`12强化学习`
- 不知道怎么标注数据，可定义什么是成功，什么是好
- 如下围棋
	`8异常检测
- 比如一个二分类的网络，输入一个异常数据，能够检测出两类都不是
- 让机器具备说“我不知道”的能力
	`9可解释的ai`
- 给出机器觉得重要的地方，给出特征点
	`10模型攻击`
- 给输入叠加噪声，使模型结果出现大的变化
	`11迁移学习`
- 解决手写数字辨识中彩色图像正确率比黑白图像低得多的问题
	`13模型压缩
	`14life-long learning
	`15meta-learning`
- 让机器学习如何学习
- 实现few-shot learning
### 002
	 机器学习的两大任务
- 回归regression
- 分类classification
- Structured Learning-输出文章/图像
## 003
实践方法论
+ 模型偏差
	+ 想在大海捞针，但是针根本不在海里
	+ 针：损失低的函数
	+ 解决：给予模型更多的灵活性，给予更多特征
+ 优化问题
	+ 卡住了，找不到损失低的函数；可能模型更大，反而损失更高
	+ 通过比较不同的模型来判断模型够不够大
	+ 先跑一些比较小的简单的模型，如线性模型，支持向量机，确认损失的大致范围
+ 过拟合
	+ 训练集上表现好，测试集上表现不好
	+ 解决：
		+ 增加数据集，即数据增强，比如左右反转，随机裁剪
		+ 给模型一些限制,让模型不要有过大的灵活性
			+ 给模型比较少的参数
			+ 更少的特征
			+ 早停(early stopping)、正则化(regularization)和丢弃法(dropout  method)。
			+ BUT！给模型太大的限制,大到有了模型偏差的问题
+ 交叉验证
	+ 把训练的数据分成两半,一部分称为训练集(training set),  一部分是验证集(validation set)。
	+ k折交叉验证
+ 不匹配
	+ 不匹配是  指训练集跟测试集的分布不同,训练集再增加其实也没有帮助了
	+ 比如如果今天用 2020 年当训练集,2021 年当测试集,根本预测不准。
	+ ![[Pasted image 20250714231724.png]]
+ Kaggle中私人和公开的数据集
## 004 优化
- 优化失败？
	- loss不再下降，卡在critical point临界点，包括**local minimal局部最小值**和**saddle point鞍点**
	- 怎么判断是local minimal还是saddle point
		- 简而言之：根据两次微分，前者周边无路可走，后者周边还是有让loss下降的路径
		- ![[Pasted image 20250715115909.png]]
		- ![[Pasted image 20250715120042.png]]
		- critical point附近第二项为0
		- ![[Pasted image 20250715120538.png]]
		- eigen values：H的特征值
		- ![[Pasted image 20250715121440.png]]
		- 实际上并没有这么做逃离鞍点的，因为计算量太大
		- ![[Pasted image 20250715122305.png]]
		- 很少有完全卡在local minimal的情况，一般loss不在下降，就直接默认是鞍点，还有让loss下降的路可以走

### batch size批次：
- 为什么要分batch？
	- ![[Pasted image 20250715133447.png]]
	- 优点：比较noisy，优化比较容易；testing上同样占优势
	- 缺点：考虑并行运算时，耗时略长
	- 需要挑选适中的batch size，即超参数
	- 鱼与熊掌？
		- ![[Pasted image 20250715133814.png]]

### momentum
- 仿照物理世界，加入“动量”的影响
- 一般的梯度下降SGD：
- 每次计算梯度，按照梯度的反方向前进
- ![[Pasted image 20250715134721.png]]
- 梯度下降+momentum SGDM：
	- 每次计算梯度，加上上次移动的方向，得出此次移动的方向
	- 优点：可以逃出local minimal或saddle point
- ![[Pasted image 20250715134751.png]]
### 自适应学习率adaptive learning rate
- 即根据当前实时的梯度调整学习率
- 训练卡住了but梯度不是零：
	- ![[Pasted image 20250715135619.png]]
	- 可能是学习率过大，在反复横跳
- 可能在不同的维度gradient相差好几个数量级，如果学习率过高，导致震荡，学习率过低，导致收敛非常非常慢
	- 请看VCR![[Pasted image 20250715140416.png]]
- 其中一种计算方式
	- Adagrad
	- ![[Pasted image 20250715140844.png]]
- 但是如果同一个维度上也需要不同的参数呢？
	- RMSProp
	- 另一种计算方式![[Pasted image 20250715144545.png]]
- Adam=RMSProp+Momentum
- leaning rate scheduling
	- η变为关于时间的函数
	- learning rate decay
		- η随时间减小![[Pasted image 20250715145408.png]]
	- warn up
		- 先变大后变小
		- ![[Pasted image 20250715145922.png]]
		- 此事在[RAdam](https://arxiv.org/abs/1908.03265)亦有记载
- 更多的optimization：
	- 不同的方法计算σ，计算gradient，以及不同的leaning rate scheduling
## 005 分类
- 一般做法：用1-n表示n个类别
	- 缺点：数字相近表示关系相近，但实际上可能不相近
- 好的做法：用单独的一个n维向量，即独热向量
	- 不会有上述缺点
- softmax：将结果放在0-1的范围
	- ![[Pasted image 20250715233930.png]]
- 交叉熵损失cross  entropy loss
	- ![[Pasted image 20250715234202.png]]
	- 比均方误差好
	- 最小化交叉熵=最大化似然
## 006 批量归一化
- 不同维度上的梯度相差很大的情况下，缩小差距，和自适应学习率起类似的作用
- 经过Batch Normalization，各个特征的分布范围变得接近，并且关联性增强
- ![[Pasted image 20250722222504.png]]
	- ![[Pasted image 20250722222528.png]]
- Batch Normalization而不是整个数据集的Normalization
- 内部协变量偏移？
	- 认为Batch Normalization能改善性能的一个可能的原因，但是被证伪了
	- ![[Pasted image 20250722222134.png]]
- 一大把的Batch Normalization
	- ![[Pasted image 20250722222019.png]]
- CNN中怎么做Batch Normalization？
- eval/test/推理时哪来均值和标准差？
	- 滑动平均值
	- ![[Pasted image 20250722223105.png]]
## 007 CNN
- 已经理解了，晚会问ai
- 如果用全连接层的方式处理图像，参数量将非常大，也很容易过拟合，因此需要做一些简化
- 观察一：给图片分类，可通过检测一些固定的模式
- 简化一：感受野
	- 感受野通常要相连
	- 核大小：长 * 宽，通道数一般和图像相同
	- 填充：放置漏掉图像边缘部分的pattern，用零填充或平均填充等来代替
- 观察二：同样的pattern可能出现在图像的各个位置
- 简化二：共享参数
	- 感受野+共享参数=滤波器（卷积层），滤波器按照步频滑动，做内积，能够检测图像中的pattern，
	- 也即卷积操作
	- 多层卷积，把前一层卷积的输出（特征映射）当作新的输入，通道数=滤波器数
	- 网络叠得越深,同样是 3 × 3 的大小的滤波器,它看的范围就会越来越大。所以网 络够深,不用怕检测不到比较大的pattern
- 观察三：下采样不影响模式检测
	- 下采样：例如把图片的奇数行，偶数列都拿掉，图片表示的物体没有大的变化
- 简化三：池化polling（汇聚）
	- 最大池化：取一组数据中的最大值
	- 平均池化：取一组数据中的平均值
	- 一般每几次卷积就加一次池化
	- 池化对性能会有影响
- 在下围棋领域的应用
	- 相当于19 * 19 * 48大小的图像
		- 48是棋子的状态个数，由围棋高手设计
	- 不能做池化：每一个棋子的作用都不小
	- 滤波器大小为5 * 5
- 传统CNN不能解决图像大小，旋转的问题
## 008 RNN
## 009 Transformer

## Colab使用技巧
- !和%
	- shell的命令之前加
	- 对于操作文件的指令，前边加%
- 挂载Google云盘的文件
```
from google.colab import drive
drive.mount('/content/drive')
```
![[Pasted image 20250723142931.png]]
- 查看分配到的gpu
```
!nvidia-smi
```
- 下载google云盘的文件
```
!gdown --id 'ID' --output pikachu.png
```