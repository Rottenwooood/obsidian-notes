---
title: "TinyVLA：面向机器人操控的快速、数据高效、视觉-语言-动作模型"
source: "https://zhuanlan.zhihu.com/p/833511695"
author:
  - "[[黄浴​​自动驾驶话题下的优秀答主]]"
published:
created: 2025-08-04
description: "24年9月来自华东师范大学、上海大学、Syracuse大学和北京人形机器人创新中心的论文“TinyVLA: Towards Fast, Data-Efficient Vision-Language-Action Models for Robotic Manipulation”。 视觉-语言-动作 (VLA) …"
tags:
  - "clippings"
---
![TinyVLA：面向机器人操控的快速、数据高效、视觉-语言-动作模型](https://picx.zhimg.com/70/v2-f83ed49f58b96ca50a393d9c10cc41c9_1440w.avis?source=172ae18b&biz_tag=Post)

TinyVLA：面向机器人操控的快速、数据高效、视觉-语言-动作模型

![](https://pic4.zhimg.com/v2-0e4d36864d132d3ddc2124b04350648f_1440w.jpg)

24年9月来自华东师范大学、上海大学、Syracuse大学和北京人形机器人创新中心的论文“ TinyVLA: Towards Fast, Data-Efficient Vision-Language-Action Models for Robotic Manipulation”。 视觉-语言-动作 (VLA) 模型通过端到端学习过程在视觉运动控制和指令理解方面表现出巨大潜力。然而，当前的 VLA 模型面临着重大挑战：它们在推理过程中速度很慢，并且需要对大量机器人数据进行大量预训练，这使得实际部署变得困难。TinyVLA 是一种紧凑型 视觉-语言-动作模型 系列，与现有的 VLA 模型相比，它具有两个关键优势：(1) 推理速度更快，(2) 数据效率更高，无需预训练阶段。该框架结合构建 TinyVLA 的两个基本组件：(1) 使用稳健、高速的 多模态模型 初始化策略主干，(2) 在微调过程中集成 扩散策略解码器 ，以实现精确的机器人动作。在模拟和真实机器人上对 TinyVLA 进行广泛的评估，结果表明，该方法在速度和数据效率方面明显优于开源 VLA 模型 OpenVLA ，同时提供相当或更优异的性能。此外，TinyVLA 在各个维度上都表现出强大的泛化能力，包括语言指令、新物体、未见过的位置、物体外观的变化、背景变化和环境变化，通常可以匹敌或超过 OpenVLA 的性能。 由于数据有限以及学习物理运动的困难，训练多任务机器人模仿者在复杂和不确定的环境中操作，面临着相当大的挑战\[1\]，\[2\]，\[3\]。此外，传统的机器人模型难以适应新的场景和任务，并且很容易受到干扰、光照条件和背景变化的影响\[4\]，\[5\]。现代方法通常利用现成的大语言模型（LLM）\[6\]，\[7\]，\[8\]，\[9\]，\[10\]，\[11\]进行场景描述以生成目标affordance，位置或热图，然后使用预定义的运动规划器完成任务\[12\]，\[13\]，\[14\]，\[15\]，\[16\]。 最近，视觉-语言-动作（VLA）模型能够使用下一个token预测方法将预训练的视觉-语言模型扩展到机器人，为此备受关注。 RT-2 \[17\] 和 OpenVLA \[18\] 等著名方法在多任务学习和泛化方面表现出色。然而，这些方法有一个关键的缺点：推理速度极慢，这主要是因为它们依赖于大型视觉语言模型和自回归动作token生成。在机器人技术中，推理速度对于使机器人能够即时响应用户查询至关重要，直接影响用户体验和机器人的整体效率。除了推理挑战之外，这些模型还需要在大型机器人数据集上进行大量的预训练。例如，OpenVLA 是在 970K 样本 OpenX 数据集 \[19\] 上进行预训练的，这使得训练的计算成本既昂贵又耗费资源。面对这些挑战，一个自然而然的问题出现了：如何构建 VLA 模型，既能保留现有 VLA 模型的优势，又能兼具快速性和数据效率？ 如图所示：TinyVLA 包含几个关键设计：1）采用预训练的多模态模型作为策略网络的初始化；2）在训练机器人数据期间，冻结预训练的部分并利用参数高效的微调技术 LoRA \[45\]，其中可训练参数仅占整个模型的 5％；3）引入一个策略解码器，该解码器通过简单但有效的线性投影连接到预训练的多模态模型，并输出机器人的可执行动作。 TinyVLA需要一种方法来表示动作空间以控制机器人。一种方法是对动作使用离散tokens，就像在 RT-2 中所做的那样。然而，对连续或高维数据使用tokens已被证明对训练极具挑战性 \[47\]，需要大量数据 \[48\]、\[49\]，并且倾向于收敛到单一状态 \[50\]。因此，这里不是将动作转换为token空间，而是利用策略头进一步学习机器人的动作空间。 具体来说，利用扩散策略 (DP) \[4\]。DP 使用去噪扩散概率模型 ( DDPM ) \[51\] 制定机器人策略，该模型主要涉及两个过程：添加噪声和去噪。在训练期间，从 0 到 N 中选择一个随机值 K，表示将高斯噪声添加到原始动作的次数。随后，DP 通过预测添加的噪声并将其减去以获得去噪动作来对噪声动作进行去噪。在推理阶段，从高斯分布采样的纯噪声 aN 开始，DP 生成一系列中间动作，{an，an−1，...，a1}，直到形成所需的无噪声输出 a0。整个模型结构如上图右侧所示，通过两个简单的线性投影和一个 LayerNorm 直接连接 DP 和多模态模型主干。多模态模型主干联合编码当前观测值和语言指令，生成控制 DP 去噪过程的多模态嵌入作为条件。 如图是实验中的单臂 Franka 和双手 UR5 的真实机器人设置：单臂场景通过固定在机器人两侧的两个外部 ZED 2 立体摄像头感知，双手机器人的场景由手腕上的两个摄像头和顶部的额外摄像头捕捉，这些摄像头是 Realsense D435i。 仿真环境是 MetaWorld 。MetaWorld \[52\] 中的 50 个任务可分为多个级别 \[53\]，即简单、中等、困难和非常困难。

[所属专栏 · 2025-05-26 09:22 更新](https://zhuanlan.zhihu.com/c_1156964299418189824)

[![](https://pic1.zhimg.com/v2-2f2f2cb9ba032c40fb8fccf86db5ae76_720w.jpg?source=172ae18b)](https://zhuanlan.zhihu.com/c_1156964299418189824)

[深度学习在计算机视觉的应用](https://zhuanlan.zhihu.com/c_1156964299418189824)

[

黄浴

自动驾驶话题下的优秀答主

439 篇内容 · 27413 赞同

](https://zhuanlan.zhihu.com/c_1156964299418189824)

[

最热内容 ·

深度学习在计算机视觉领域（包括图像，视频，3-D点云，深度图）的应用一览

](https://zhuanlan.zhihu.com/c_1156964299418189824)

编辑于 2025-02-14 15:53・北京[机器人](https://www.zhihu.com/topic/19551273)[视觉语言大模型](https://www.zhihu.com/topic/28271953)[策略](https://www.zhihu.com/topic/19599901)

![](https://picx.zhimg.com/v2-862d6f4f2ab5c651614aadf3721c7c9e_l.jpg?source=32738c0c&needBackground=1)

发首评

还没有评论，发表第一个评论吧