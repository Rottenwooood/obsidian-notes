

# VLA模型在具身智能中的合成数据方法：内容、技术与训练阶段

## 1. 核心挑战与总体策略

### 1.1 具身智能数据获取的核心挑战

具身智能（Embodied AI）的发展，特别是以视觉-语言-动作（VLA）模型为核心的通用机器人系统，面临着前所未有的数据挑战。与纯粹基于语言的模型可以从海量互联网文本中学习不同，VLA模型需要理解并与物理世界进行交互，这对其训练数据提出了独特且严苛的要求。这些挑战主要体现在数据采集的成本、数据的维度与多样性，以及异构数据的融合三个方面，共同构成了当前具身智能发展的核心瓶颈。

#### 1.1.1 物理世界数据采集的高昂成本与规模限制

在物理世界中为机器人采集训练数据是一项极其耗时且昂贵的工作。首先，机器人硬件本身成本高昂，且操作复杂、速度有限、易于损坏，这极大地限制了数据采集的效率和规模 。例如，让人形机器人执行一个简单的“整理桌面”任务，可能需要研究人员花费大量时间进行遥操作或监督学习，而机器人完成动作的速度远低于人类。其次，真实世界的数据采集过程难以并行化，无法像处理文本或图像数据那样进行大规模分布式处理。每一个任务演示都需要物理空间、设备和人力，这使得构建一个覆盖广泛任务和场景的大规模真实世界数据集变得不切实际。因此，尽管真实数据在反映物理规律方面具有不可替代的价值，但其高昂的成本和有限的规模，严重制约了VLA模型的训练和发展，使其难以达到类似大型语言模型（LLM）那样的数据规模和性能水平 。

#### 1.1.2 数据维度与场景的多样性匮乏

VLA模型需要具备强大的泛化能力，即能够将在训练数据中学到的知识和技能，迁移到全新的、未见过的环境中。然而，真实世界数据采集的困难直接导致了训练数据在维度和场景上的多样性严重不足。研究人员很难在有限的预算和时间内，采集到覆盖各种光照条件、背景环境、物体材质、摆放方式以及任务变体的数据 。例如，一个用于抓取杯子的模型，如果只在一个固定的实验室环境中，使用同一种杯子进行训练，那么它在面对不同形状、大小、材质或在不同光照、背景下的杯子时，很可能表现不佳。这种数据多样性的匮乏，使得模型容易过拟合到特定的训练场景，导致其在真实世界应用中的鲁棒性和适应性大打折扣。因此，如何低成本、高效率地生成具有高度多样性的训练数据，成为提升VLA模型泛化能力的关键。

#### 1.1.3 异构数据（视觉、语言、动作）融合的复杂性

VLA模型的核心在于其能够处理和融合来自不同模态的信息，即视觉（Vision）、语言（Language）和动作（Action）。这要求训练数据必须是高质量、时间同步且语义对齐的多模态数据。例如，一个“将苹果放在盘子里”的指令，需要与机器人执行该动作时的视觉观察（看到苹果和盘子）和动作序列（移动手臂、张开夹爪、闭合夹爪、移动、释放）精确对应。采集和标注这样的异构数据极具挑战性。首先，需要确保不同传感器（如摄像头、力矩传感器、关节编码器）的数据在时间上严格同步。其次，需要为每个动作序列提供准确、多样的语言指令描述，这可能需要大量的人工标注工作。最后，如何将这些不同来源、不同格式的数据有效地整合到一个统一的框架中，供模型进行端到端的学习，也是一个复杂的技术难题。这种异构数据融合的复杂性，进一步增加了构建大规模、高质量VLA训练数据集的难度 。

### 1.2 合成数据的核心价值与总体策略

面对物理世界数据采集的重重困难，合成数据（Synthetic Data）应运而生，并迅速成为推动具身智能发展的关键路径。合成数据通过在虚拟环境中模拟物理世界，能够以极低的成本、极高的效率生成海量、多样化的训练数据，从而有效弥补真实数据的不足。其核心价值不仅在于“量”的扩充，更在于“质”的提升，为VLA模型的训练和验证提供了前所未有的灵活性和可控性。

#### 1.2.1 作为真实数据的“放大器”与补充

合成数据最核心的价值在于其能够作为真实数据的“放大器”。通过利用少量高质量的真实数据作为“种子”，结合先进的仿真和生成技术，可以创造出数量庞大、变化丰富的“衍生”数据。例如，英伟达（NVIDIA）的具身智能实验室提出的**MimicGen系列方法**，可以从少量人类示范中，通过在仿真环境中变换物体位置、姿态和场景布局，自动生成成千上万条新的、但物理上合理的机器人运动轨迹 。这种方法不仅极大地降低了数据采集的成本，还有效地解决了数据多样性不足的问题。此外，合成数据还可以补充真实数据中难以采集或危险的场景，例如机器人与易碎物品的交互、在极端环境下的操作等。通过这种方式，合成数据与真实数据形成了完美的互补，共同构建了一个更加全面、鲁棒的训练数据体系，为VLA模型的泛化能力提供了坚实的基础。

#### 1.2.2 实现从“离散模块化”到“端到端”的训练范式转变

传统的机器人学习往往采用“离散模块化”的方法，即将复杂的任务分解为感知、规划、控制等多个独立的模块，分别进行训练和优化。这种方法虽然在某些特定任务上有效，但模块间的通信和协作带来了巨大的复杂性，且难以实现全局最优。VLA模型的兴起，标志着一种 **“端到端”训练范式**的转变，即直接从原始传感器输入（如图像）和高层语言指令，映射到机器人的底层动作输出。这种范式要求训练环境能够提供一个集成的、一站式的平台，能够同时模拟物理动力学、传感器渲染、物体交互等多个方面。高精度物理仿真平台，如**NVIDIA的Omniverse Isaac Sim**，正是为了满足这一需求而设计的 。它将物理仿真、渲染、传感器模拟等功能整合在一起，为VLA模型提供了一个理想的端到端训练环境，从而极大地简化了开发流程，并有望实现更优的整体性能。

#### 1.2.3 物理数据与仿真合成数据的协同配比趋势

随着合成数据技术的不断成熟，业界逐渐形成了一种共识：未来的具身智能模型训练，将不再单纯依赖物理数据或仿真数据，而是采用二者协同的混合数据策略。一个被广泛引用的预测是，**物理数据与仿真合成数据的比例可能会达到1:8，甚至更高** 。这意味着，仿真合成数据将在模型训练中占据主导地位。这种趋势的背后，是两种数据类型的优势互补。物理数据提供了最真实的物理规律和传感器噪声，是模型验证和最终部署的“试金石”。而仿真合成数据则提供了大规模、多样化、可标注的训练样本，是模型进行高效学习和泛化能力培养的“主战场”。通过在仿真环境中进行大规模的预训练和微调，再结合少量真实数据进行校准和验证，可以构建出既强大又可靠的VLA模型。这种协同配比的趋势，正在成为当前具身智能领域的主流技术路线。

## 2. 基于高精度物理仿真的合成数据生成

基于高精度物理仿真的合成数据生成是当前解决具身智能数据稀缺问题最主要、最有效的技术路径之一。通过在计算机中构建一个与真实世界物理规律高度一致的虚拟环境，研究人员可以安全、高效、低成本地生成海量、多样化且自带精确标注的训练数据。这种方法不仅能够模拟各种复杂的物理交互，还能对光照、材质、场景布局等进行精细的控制和随机化，从而极大地提升VLA模型的泛化能力和鲁棒性。

### 2.1 核心平台与引擎

实现高质量合成数据生成的关键在于强大的物理仿真平台和引擎。这些平台需要提供高精度的物理模拟、逼真的视觉渲染、丰富的传感器模型以及灵活的API接口，以支持复杂的机器人学习任务。

#### 2.1.1 NVIDIA Omniverse Isaac Sim：端到端综合性仿真平台

**NVIDIA的Omniverse Isaac Sim**是当前具身智能仿真领域的标杆平台。它基于NVIDIA的Omniverse技术构建，提供了一个端到端的综合性仿真解决方案，完美契合了VLA模型训练的需求 。Isaac Sim的核心优势在于其高度的集成性和可扩展性。它将多个关键功能模块整合在一个统一的框架内，包括：

*   **高精度物理仿真**：基于NVIDIA PhysX，能够模拟刚体、软体、流体等多种物理现象，确保机器人与环境的交互符合真实世界的物理规律。
*   **逼真视觉渲染**：利用RTX实时光线追踪技术，能够生成具有复杂光影、反射和折射效果的高保真图像，有效缩小仿真到现实（Sim2Real）的视觉差距。
*   **丰富的传感器模型**：内置了多种常用机器人传感器的精确模型，如RGB-D相机、激光雷达（LiDAR）、IMU等，可以模拟传感器的噪声、畸变和延迟，为VLA模型提供逼真的感知输入。
*   **可扩展的API和工具链**：提供了Python API，允许研究人员方便地创建、修改和控制仿真环境，并与主流的深度学习框架（如PyTorch、TensorFlow）无缝集成。

通过Isaac Sim，研究人员可以构建复杂的虚拟场景，定义多样化的任务，并大规模并行地生成训练数据，极大地加速了VLA模型的研发周期。目前，包括**1X、波士顿动力、宇树科技**在内的众多领先机器人公司都已加入NVIDIA的生态系统，利用Isaac Sim进行模型训练和测试 。

#### 2.1.2 其他物理引擎：MuJoCo, Genesis等

除了NVIDIA Isaac Sim，学术界和工业界还存在其他优秀的物理引擎，它们在特定方面具有独特的优势，并被广泛应用于机器人学习研究中。

*   **MuJoCo (Multi-Joint dynamics with Contact)** ：由Google DeepMind收购并开源，MuJoCo以其高效的接触动力学计算和稳定的数值积分而闻名。它特别适用于需要精确模拟机器人与物体接触、碰撞的任务，如灵巧手操作、行走等。MuJoCo的计算速度非常快，使其成为强化学习研究的理想选择，许多经典的机器人学习算法和基准测试（如OpenAI Gym）都基于MuJoCo构建 。

*   **Genesis**：这是一个于2024年12月由多个组织联合发布的、相对较新的物理引擎。Genesis旨在成为一个高性能、轻量级的生成式物理引擎，专注于为通用机器人学习提供一个统一的模拟平台 。它支持多种材料的模拟，并完全支持可微分特性，这意味着可以方便地将其集成到基于梯度的优化和深度学习流程中。Genesis的目标是打造一个能够生成精确物理世界的平台，从而为机器人解锁无限且多样化的数据，加速机器人技能的研发 。

这些平台与NVIDIA Isaac Sim共同构成了当前具身智能仿真的技术生态，为研究人员提供了多样化的工具选择，以满足不同研究任务的需求。

### 2.2 关键技术路径：从人类示范到大规模数据

如何从有限的、高质量的人类示范中，自动地、大规模地生成新的、有效的训练数据，是仿真数据生成的核心技术挑战。英伟达具身智能实验室提出的**MimicGen系列方法**，为此提供了一套行之有效的解决方案 。

#### 2.2.1 轨迹复现与泛化：MimicGen系列方法

MimicGen系列方法的核心思想是“举一反三”，即通过分析少量人类示范的内在结构和逻辑，将其在新的场景中进行复现和泛化，从而自动化地生成大量新的演示数据。

##### 2.2.1.1 MimicGen：分割并复现人类示范轨迹

MimicGen是该系列的基础方法。它的工作流程可以概括为以下几个步骤 ：

1.  **分割（Segmentation）** ：首先，将一段完整的人类示范轨迹，根据其与环境中物体的交互关系，分割成一系列以目标物体为中心的短片段。例如，一个“将杯子从桌子A拿到桌子B”的任务，可以被分割为“移动到桌子A”、“抓取杯子”、“移动到桌子B”、“释放杯子”等几个片段。
2.  **变换（Transformation）** ：然后，在仿真环境中，对场景中的物体（如桌子、杯子）进行随机的位置、姿态变换，生成一个全新的场景布局。
3.  **复现（Reproduction）** ：最后，将分割好的轨迹片段，在新的场景布局中进行复现。系统会根据物体的新位置，自动调整机器人手臂的运动轨迹，确保其能够准确地完成每个子任务（如准确地抓取到新位置的杯子）。

通过这种方式，MimicGen能够利用一条人类示范，在成百上千个不同的场景布局中，生成成百上千条新的、但逻辑一致的训练轨迹，极大地扩充了数据集的规模和多样性。

##### 2.2.1.2 SkillMimicGen：从少量示范生成大量高质量演示

SkillMimicGen是MimicGen的扩展和深化，其核心目标是“从少量示范中生成大量高质量的演示数据集” 。与基础版MimicGen相比，SkillMimicGen可能引入了更复杂的技能抽象和组合机制。它不仅仅是简单地复现轨迹，而是试图理解示范中蕴含的“技能”（Skill），例如“抓取”、“放置”、“推动”等。然后，它可以将这些基本技能进行重新组合，生成完成更复杂任务的全新轨迹。例如，通过学习“抓取杯子”和“打开水龙头”两个技能，SkillMimicGen可能能够生成“用杯子接水”的全新演示。这种方法使得数据生成更具创造性和泛化性，能够为机器人学习提供更丰富、更多样化的数据支持。

##### 2.2.1.3 DexMimicGen：合成类人灵巧手的运动轨迹

DexMimicGen是MimicGen系列中专门针对高自由度、复杂操作任务（如类人灵巧手操作）的增强版本 。灵巧手的控制是机器人领域的一大难题，其高维度的动作空间和复杂的接触动力学使得数据采集尤为困难。DexMimicGen通过结合高精度物理仿真和先进的轨迹优化算法，能够基于少量人类演示，合成出大量逼真且物理上可行的灵巧手运动轨迹。据报道，该方法能够实现高达**200倍的数据增强**，不仅极大地解决了灵巧手训练数据集的获取难题，而且在实验中将机器人的任务表现提升了显著水平 。DexMimicGen的成功，展示了基于仿真的数据增强技术在处理复杂、高维度机器人任务方面的巨大潜力。

#### 2.2.2 场景与物体的参数化变换

为了进一步提升合成数据的多样性，研究人员还会对仿真场景和物体进行大规模的参数化变换，即**域随机化（Domain Randomization）** 。

##### 2.2.2.1 物体姿态与位置的随机化

这是最基本也是最有效的随机化手段之一。在每次生成数据时，系统会随机改变场景中所有可移动物体的位置和姿态。例如，在桌面整理任务中，杯子、盘子、书本等物品的摆放位置、朝向都会被随机化。这迫使VLA模型学习物体的空间关系，而不是记住特定的物体位置，从而提升其在真实世界中对杂乱环境的适应能力。

##### 2.2.2.2 光照、材质与纹理的多样化

视觉外观的随机化对于缩小Sim2Real的视觉差距至关重要。系统会随机改变场景中的光照条件（如光源位置、强度、颜色）、物体的材质属性（如金属度、粗糙度）以及表面的纹理图案。例如，同一个桌子，在不同的数据样本中可能呈现出木质、金属或塑料的质感，并可能在不同的光照下产生不同的阴影和高光。这种随机化使得模型能够学习到更加鲁棒的视觉特征，对真实世界中光照变化、物体磨损等情况具有更强的适应性。斯坦福大学的iGibson仿真环境就提供了丰富的域随机化功能，以训练更具鲁棒性的具身智能体 。

### 2.3 数据内容与应用任务

通过上述方法生成的合成数据，具有内容丰富、标注精确的特点，能够支持VLA模型在多种复杂任务上的训练。

#### 2.3.1 数据内容：高保真视觉、精确动作序列、物理交互反馈

合成数据通常包含以下几个核心组成部分：

*   **高保真视觉信息**：通过光线追踪渲染生成的RGB图像，以及对应的深度图、分割图等。这些图像在视觉上与真实世界非常接近，为VLA模型提供了高质量的视觉输入。
*   **精确的动作序列**：记录了机器人完成任务所需的每一个动作，通常以关节角度、末端执行器位姿或速度等形式表示。这些动作标签是自动生成的，因此具有极高的精度和一致性。
*   **物理交互反馈**：在仿真过程中，可以记录下机器人与环境交互时产生的各种物理量，如接触力、力矩、物体速度等。这些信息可以作为额外的监督信号，帮助模型更好地理解物理规律。

#### 2.3.2 应用任务：复杂操作、长程任务、灵巧手控制

基于仿真合成数据，VLA模型可以被训练来完成各种复杂的具身智能任务：

*   **复杂操作任务**：如使用工具（螺丝刀、锤子）、组装零件、折叠衣物等，这些任务需要精细的动作控制和长期的规划能力。
*   **长程任务**：如整理整个房间、完成一道菜的烹饪流程等，这些任务由多个子任务组成，需要模型具备任务分解和长期记忆的能力。斯坦福大学的**BEHAVIOR-1K基准测试**就包含了1000个这样的家庭活动，用于评估具身AI系统的性能 。
*   **灵巧手控制**：如弹钢琴、转笔、穿针引线等，这些任务对机器人手部的灵活性和协调性提出了极高的要求，是合成数据应用的一个重要方向。

## 3. 基于生成式AI的合成数据增强

随着生成式AI技术的飞速发展，特别是大语言模型（LLM）和扩散模型（Diffusion Models）的出现，为具身智能合成数据的生成开辟了全新的路径。与基于物理仿真的方法不同，生成式AI能够直接从数据中学习复杂的分布，并创造出全新的、多样化的内容。将生成式AI与物理仿真相结合，有望突破传统方法的瓶颈，实现更高效、更具创造性的数据增强。

### 3.1 生成式AI与仿真的结合

生成式AI与物理仿真的结合，正在从根本上改变合成数据的生成方式，主要体现在以下两个方面：

#### 3.1.1 利用多模态大模型快速构建虚拟场景

传统的虚拟场景构建通常需要专业的3D建模师花费大量时间手动创建，成本高昂且效率低下。而多模态大模型，特别是那些具备**文本到3D（Text-to-3D）** 或**图像到3D（Image-to-3D）** 能力的模型，可以极大地加速这一过程。研究人员只需输入一段自然语言描述（如“一个阳光明媚的厨房，桌上有一个红色的苹果和一个白色的盘子”）或一张参考图片，模型就能自动生成一个对应的、具有合理布局和物理属性的3D场景。例如，李飞飞团队提出的 **“数字表亲”（Digital Cousin）** 概念，就是利用大模型来构建与真实世界场景在几何和语义上相似，但并非完全相同的虚拟场景，用于模型训练 。这种方法不仅大幅降低了场景构建的门槛和成本，还能生成大量多样化的训练环境，有效提升模型的泛化能力。

#### 3.1.2 生成式AI作为数据“放大器”扩展真实数据

生成式AI可以作为强大的数据“放大器”，将有限的真实数据扩展为大规模、多样化的虚拟数据。例如，英伟达在其**GR00T N1人形机器人基础模型**的训练中，就采用了一种新颖的方法：利用最先进的视频生成模型来“幻觉”（hallucinate）出新的合成数据 。具体来说，他们首先收集一些真实的人类遥操作数据，然后训练一个视频生成模型（如扩散模型）来学习这些数据的分布。之后，这个模型就可以生成无数新的、在像素级别上具有准确物理特性的视频数据。这些由AI生成的视频，可以作为额外的训练数据，与真实数据和传统仿真数据一起，共同用于训练VLA模型。这种方法的优势在于，它能够生成传统物理仿真难以模拟的、高度逼真和复杂的动态场景，从而进一步丰富训练数据的分布。

### 3.2 核心技术与方法

在生成式AI与仿真结合的领域，涌现出了一些具有代表性的核心技术和方法，它们正在推动合成数据生成向着更智能、更高效的方向发展。

#### 3.2.1 “数字表亲”（Digital Cousin）概念

“数字表亲”是由李飞飞及其研究团队于2024年10月提出的一个创新性概念，旨在解决Sim2Real迁移中的泛化问题 。

##### 3.2.1.1 与数字孪生的区别

传统的 **“数字孪生”（Digital Twin）** 追求的是与真实物体或场景在几何、物理属性上的精确一一对应。然而，这种精确复制往往会导致模型过拟合到特定的实例，当面对真实世界中微小的变化时，性能就会下降。而 **“数字表亲”** 则采取了不同的思路。它并不要求与真实物体完全相同，而是追求在更高层次上的“相似性”——即具有相似的几何形状、语义类别和功能属性 。例如，一个真实世界中的宜家“Billy”书柜，它的“数字孪生”会是一个一模一样的虚拟复制品；而它的“数字表亲”则可能是一个在尺寸、颜色、材质上略有不同，但同样具有“书柜”功能的虚拟物体。

##### 3.2.1.2 提升模型在域外的泛化能力

通过在由“数字表亲”构成的多样化虚拟场景中进行训练，VLA模型能够学习到更加泛化的概念和技能，而不是记住特定实例的细节。研究表明，利用“数字表亲”训练的策略，相比于在“数字孪生”上训练的策略，具有更卓越的**域外（out-of-domain）泛化能力**，同时还能保持可媲美的**域内（in-domain）性能**。更重要的是，这种方法支持**零样本（zero-shot）的模拟到现实策略迁移**，即模型在仿真环境中训练后，无需在真实世界中进行任何微调，就能直接部署并表现出色 。这为构建真正通用的具身智能系统提供了一条极具前景的路径。

#### 3.2.2 生成式3D世界模型

构建能够理解和生成3D物理世界的模型，是生成式AI在具身智能领域的终极目标之一。这类模型旨在直接从数据中学习物理规律，并能够生成可交互的、动态的3D环境。

##### 3.2.2.1 李飞飞World Labs的空间智能模型

2024年12月，由李飞飞创立的**World Labs**发布了其首个“空间智能”模型 。该模型的强大之处在于，它仅需一张2D图片作为输入，就能生成一个逼真的、可交互的3D世界。与只能生成视频的模型不同，World Labs的模型能够理解图片中不同景物的深度和相对关系，并生成一个更加逼近物理世界真实几何结构的3D环境。这不仅仅是生成一个可互动的视频，而是更接近于对图片物理关系的深刻理解。这种能力对于具身智能至关重要，因为它可以为机器人提供一个无限、多样的虚拟训练场，让机器人在其中学习导航、操作和与复杂环境交互的技能。

##### 3.2.2.2 谷歌Gene2与OpenAI Sora的应用潜力

几乎在同一时期，谷歌DeepMind发布了新一代世界模型**Genie 2**，而OpenAI也发布了其视频生成模型**Sora** 。这些模型同样展示了从单张图片或文本生成多样化、可交互3D环境或长视频的惊人能力。尽管它们目前主要生成的是2D视频内容，但其背后蕴含的对物理世界的理解和模拟能力，使其在具身智能领域具有巨大的应用潜力。未来，这些模型有望被用于快速生成大规模的、带有丰富动态交互的视频数据，用于VLA模型的预训练，帮助模型建立起对物理世界基本规律的“常识”。

### 3.3 数据内容与应用任务

基于生成式AI的合成数据，其内容和应用任务也呈现出新的特点。

#### 3.3.1 数据内容：多样化的3D场景、可交互的物理世界

与基于物理仿真的数据相比，生成式AI能够创造出更加多样化、更具想象力的3D场景和物理世界。它不再局限于手动构建的场景，而是可以从海量数据中学习并生成全新的、前所未见的组合。这些数据内容不仅包括静态的场景布局，更重要的是包含了动态的、可交互的物理过程，如物体碰撞、流体流动、柔性物体变形等。

#### 3.3.2 应用任务：零样本模拟到现实的策略迁移、复杂环境导航

基于生成式AI合成数据训练出的VLA模型，其核心优势在于其强大的泛化和迁移能力。

*   **零样本模拟到现实的策略迁移**：通过在由“数字表亲”或生成式3D世界模型构建的、极度多样化的虚拟环境中进行训练，模型能够学习到非常鲁棒的策略，从而实现在真实世界中的零样本部署，无需任何额外的微调 。
*   **复杂环境导航与探索**：生成式AI可以创造出各种复杂、动态甚至非结构化的环境，如灾后现场、外星地表等。在这些环境中训练机器人，可以极大地提升其在未知和危险环境中的自主导航和探索能力。

## 4. 低成本真实数据采集与增强方法

尽管合成数据在规模和多样性上具有巨大优势，但真实世界的数据对于验证模型、校准仿真以及学习那些难以精确模拟的细微物理交互仍然至关重要。然而，传统真实数据采集方法成本高昂、效率低下。为了解决这一矛盾，学术界，特别是斯坦福大学的研究团队，提出了一系列创新的低成本真实数据采集与增强方法。这些方法旨在通过巧妙的硬件设计和算法创新，大幅降低数据采集的门槛，使大规模、高效率的真实数据收集成为可能。

### 4.1 低成本遥操作数据采集系统

遥操作（Teleoperation）是采集高质量机器人演示数据最直接的方式。通过让人类操作员直接控制机器人完成任务，可以自然地获得与任务成功执行相对应的视觉和动作数据。然而，传统的遥操作设备（如专业级的力反馈手柄、VR系统）价格昂贵，且操作复杂。为此，斯坦福团队开发了两种极具代表性的低成本遥操作数据采集系统。

#### 4.1.1 斯坦福Mobile Aloha系统

**Mobile ALOHA**（A Low-cost Open-source Hardware System for Bimanual Teleoperation）是斯坦福团队在2024年发布的一个低成本、开源的双臂移动操作机器人系统 。它的核心创新在于将原有的静态ALOHA双臂系统安装在一个可由人类操作员直接推动的轮式基座上，从而实现了对机器人全身的遥操作。

*   **硬件设计与工作原理**：操作员通过双手直接操控ALOHA的两个机械臂，同时通过身体推动整个基座来移动机器人的位置。系统会同时记录基座的移动速度数据和手臂的关节角度数据，形成一个**16维的动作向量**（14个关节位置 + 2个基座速度） 。这种“身体力行”的控制方式非常直观，无需复杂的培训即可上手。
*   **数据效率与性能**：Mobile ALOHA在数据效率上表现惊人。例如，在“擦拭葡萄酒”等复杂的移动操作任务上，**仅需50次人类演示**，就能通过模仿学习实现高达**90%的成功率** 。这证明了其在复杂任务中的有效性和极高的数据效率。
*   **与静态数据的联合训练**：为了进一步提升性能，Mobile ALOHA还开创性地将移动操作数据与现有的、任务不同的静态ALOHA数据集进行联合训练。尽管两个数据集的任务和机器人形态存在差异（静态ALOHA的机械臂是固定相对的，而Mobile ALOHA的是平行向前的），但联合训练依然显著提升了模型在移动操作任务上的表现，这为如何利用现有静态数据来增强移动操作能力提供了宝贵的经验 。

#### 4.1.2 斯坦福UMI（手持夹爪）系统

**UMI**（Universal Manipulation Interface）是斯坦福团队在2024年2月发布的另一个革命性的数据采集框架，其核心是一个手持的夹爪设备 。与Mobile ALOHA相比，UMI的成本更低，便携性更强，数据采集方式也更为灵活。

*   **硬件设计与工作原理**：UMI的本质是一个带有鱼眼镜头和IMU（惯性测量单元）传感器的手持夹爪。操作员手持这个夹爪，像使用自己的手一样去完成任务（如刷盘子、叠衣服）。夹爪上的鱼眼镜头和侧面镜子提供了宽广的立体视觉，记录下操作员视角的RGB视频。同时，IMU和夹爪的关节传感器记录下夹爪的运动轨迹和开合状态 。
*   **解决关键延迟问题**：UMI框架特别考虑了真实机器人部署中的三大延迟：传感器延迟、推理延迟和执行延迟。它通过一种 **“推理时延迟匹配”（inference-time latency matching）** 的技术，在数据采集时就模拟这些延迟，使得学习到的策略对真实机器人的延迟不敏感，从而可以直接部署到不同的机器人平台上 。
*   **零样本泛化能力**：通过在多样化的真实环境中采集数据（例如，在不同的厨房、使用不同的餐具刷盘子），UMI训练出的策略展现出了强大的**零样本泛化能力**。模型能够将在一个环境中学习到的技能，直接应用到全新的环境和物体上，而无需任何再训练 。UMI的开源硬件和软件设计，极大地降低了机器人模仿学习的门槛，使得普通研究者也能快速收集高质量的真实世界数据。

### 4.2 从人类视频中学习

除了遥操作，从互联网上大量存在的、未经标注的人类操作视频中学习，是另一种极具潜力的低成本数据获取方式。这种方法的挑战在于如何从这些“在野”（in-the-wild）视频中提取出可供机器人学习的有效信息。

#### 4.2.1 HumanPlus：基于视频进行全身动作数据采集

**HumanPlus**是斯坦福Mobile ALOHA团队于2024年6月推出的一个全栈人形机器人系统，其核心目标是从人类数据中高效学习复杂的自主技能 。

*   **核心组成：影子系统与模仿学习**：HumanPlus系统包含两个核心部分。第一部分是一个**实时影子系统（Shadowing System）** ，它允许人类操作员仅通过一个RGB摄像头，就能实时地全身控制人形机器人。机器人会像影子一样模仿操作员的动作，包括拳击、打乒乓球等快速、多样的运动。第二部分是一个**人形模仿Transformer（Humanoid Imitation Transformer）** ，这是一个基于Transformer的模仿学习算法，能够利用影子系统收集到的数据，高效地学习自主技能 。
*   **数据驱动的影子系统训练**：影子系统本身也是一个数据驱动的模型。研究人员首先利用一个包含40小时人类运动数据的公开数据集（AMASS），在MuJoCo等物理仿真环境中，通过强化学习训练一个底层的控制策略（Humanoid Shadowing Transformer, HST）。这个在仿真中训练好的策略，可以直接零样本迁移到真实机器人上，使其能够稳定地跟随人类动作 。
*   **高效学习与卓越性能**：通过影子系统，操作员可以方便地收集机器人在真实世界中完成各种任务的全身数据。然后，利用这些数据，模仿学习算法**仅需40次左右的演示**，就能让机器人自主完成穿鞋站立、仓库卸货、叠衣服、打字等复杂任务，**成功率高达60%-100%** 。HumanPlus系统巧妙地结合了仿真数据预训练、低成本真实数据采集和高效模仿学习，为从人类视频中学习提供了一个完整的、高效的解决方案。

#### 4.2.2 DexCap：基于动作捕捉的数据采集

**DexCap**是斯坦福大学提出的另一种数据采集方案，它利用**动作捕捉（Motion Capture）** 技术来精确记录人类手部的运动 。通过在操作员的手上佩戴动作捕捉标记点，DexCap可以毫米级的精度记录下人类在执行精细操作时的手部轨迹和姿态。这些数据对于训练机器人的灵巧手控制策略具有极高的价值。与UMI相比，DexCap能够提供更高精度的动作数据，但成本也相对更高。它适用于那些对操作精度要求极高的任务，如手术模拟、精密装配等。

### 4.3 数据增强与标注技术

无论是通过遥操作还是视频学习获得的真实数据，其规模通常仍然有限。因此，需要结合各种数据增强与标注技术，进一步扩充数据集的规模和多样性。

#### 4.3.1 算法辅助标注：深度估计、光流、3D注释

对于从视频中提取的数据，可以利用计算机视觉算法进行辅助标注。例如，使用单目或双目深度估计算法，可以从RGB视频中恢复出场景的深度信息；使用光流估计算法，可以分析物体的运动模式；使用3D目标检测和姿态估计算法，可以自动标注出视频中关键物体的3D位置和姿态。这些算法辅助的标注方法，可以大大减少昂贵的人工标注成本。

#### 4.3.2 传统数据增强：几何与颜色变换

与在图像识别领域一样，传统的数据增强技术同样适用于具身智能。通过对采集到的图像和对应的动作数据进行随机的几何变换（如裁剪、旋转、缩放）和颜色变换（如亮度、对比度、饱和度调整），可以在不改变任务语义的前提下，创造出新的训练样本，从而提升模型对视觉变化的鲁棒性。例如，在Mobile ALOHA的训练中，就采用了随机裁剪和颜色抖动等图像增强技术 。

#### 4.3.3 基于模型的增强：GANs, VAEs

更高级的数据增强方法可以利用生成模型，如**生成对抗网络（GANs）** 或**变分自编码器（VAEs）** 。这些模型可以学习真实数据的分布，并生成全新的、与真实数据难以区分的样本。例如，可以训练一个GAN来生成新的桌面背景，或者生成不同光照条件下的物体外观，从而进一步丰富训练数据的视觉多样性。随着生成式AI技术的发展，这类基于模型的增强方法将在具身智能数据生成中扮演越来越重要的角色。

## 5. 多模态数据融合与指令生成

为了让VLA模型能够理解并执行人类的指令，需要将采集到的动作数据与相应的自然语言指令进行关联。这通常通过两种方式实现：基于预定义模板的指令生成和利用大语言模型生成多样化指令。

### 5.1 语言指令的生成与关联

#### 5.1.1 基于预定义模板的指令生成

基于预定义模板的指令生成是一种简单有效的方法 。研究人员可以设计一系列包含占位符的指令模板，例如“将<物体A>放到<物体B>上”或“打开<抽屉>”。然后，通过自动化的脚本，将采集数据中识别出的物体名称、位置等信息填充到这些占位符中，从而生成大量的结构化指令。这种方法的优点是生成的指令语法正确、语义清晰，且与动作数据高度对应。然而，其缺点是生成的指令风格较为单一，缺乏自然语言的多样性和灵活性。

#### 5.1.2 利用大语言模型生成多样化指令

为了使指令更加自然和多样化，可以利用**大语言模型（LLM）** 来生成指令。研究人员可以先提供一些示例指令，让LLM学习指令的风格和模式，然后让LLM基于采集到的动作和场景信息，生成多种不同的、但语义等价的指令变体 。例如，对于“将苹果放到盘子里”这个动作，LLM可以生成“请把苹果移到盘子上”、“帮我把苹果放进盘子里”等多种表达方式。这种方法能够极大地丰富指令的多样性，提升模型对自然语言的理解能力和泛化能力。**InstructVLA模型的VLA-IT数据集**中，就利用了**Gemini-2.5-Pro**来生成多样化的指令变体 。

### 5.2 大规模多模态数据集的构建

为了训练强大的VLA模型，需要构建大规模、高质量、多样化的多模态数据集。这通常通过整合现有数据集和构建新的专用数据集来实现。

#### 5.2.1 整合现有机器人操作数据集（如RLBench, CALVIN）

目前，学术界已经发布了多个开源的机器人操作数据集，如**RLBench、CALVIN、Open X-Embodiment**等 。这些数据集包含了大量的机器人操作轨迹，是构建VLA模型训练数据的重要基础。通过整合这些数据集，可以迅速扩大训练数据的规模和任务多样性。例如，**OpenVLA模型**就在**Open X-Embodiment数据集的97万个机器人操作片段**上进行了训练 。在整合过程中，需要对不同数据集的数据格式、坐标系、动作空间等进行统一和对齐，以确保数据的一致性。

#### 5.2.2 构建包含人机交互、场景描述和问答对的数据集（如VLA-IT）

除了整合现有数据，构建新的、更具针对性的数据集也至关重要。例如，为了提升VLA模型的指令跟随和推理能力，InstructVLA模型构建了**VLA-IT（Vision-Language-Action Instruction Tuning）数据集** 。该数据集包含了约**65万条人机交互数据**，不仅涵盖了多样化的任务指令，还包含了丰富的场景描述和问答对。这些数据旨在模拟机器人在复杂语境中对用户意图的理解与响应，从而将机器人操作纳入到更广泛的指令跟随体系中。这种包含多层次信息的数据集，有助于训练出既能“动手”又能“动脑”的通用VLA模型。

#### 5.2.3 整合动作捕捉、VR交互与RGB视频（如UniHand）

为了获取更加精细和多样化的人类动作数据，研究人员还探索了整合多种数据源的方法。例如，Being-H0模型为支撑其物理指令调优框架，构建了**UniHand数据集** 。该数据集整合了来自**动作捕捉设备、VR交互记录以及纯RGB视频**的数据，形成了一个包含数百万动作驱动型指令实例的大规模数据集。通过统一的物理空间对齐和部件级动作Token化技术，UniHand成功地将这些异构数据源融合在一起，为VLA模型提供了高质量、大规模的人类手部操作数据，从而架起了人类视频与机器人操作之间的桥梁 。

## 6. 合成数据在VLA模型训练阶段的应用

合成数据在VLA模型的整个训练生命周期中都扮演着至关重要的角色，从建立基础的世界模型，到适应特定任务，再到持续的优化和验证，形成了一个高效、低成本的闭环。

### 6.1 预训练阶段：建立基础世界模型

在VLA模型的训练流程中，预训练阶段的目标是建立一个通用的、对世界有基础理解的模型。合成数据在这一阶段扮演着至关重要的角色。

#### 6.1.1 利用大规模、多样化的合成数据进行预训练

由于真实数据的获取成本高昂，难以满足预训练阶段对海量数据的需求，因此，**大规模、多样化的合成数据成为了预训练的主要数据来源** 。通过在仿真环境中生成数以亿计的视觉-语言-动作配对数据，可以让VLA模型在预训练阶段就接触到极其广泛的任务和场景。例如，银河通用的**GraspVLA模型**就采用了**10亿级别的合成大数据**进行训练 。这种大规模的训练，使得模型能够学习到通用的视觉特征、语言与视觉的关联、以及基本的物理交互规律，为后续的微调打下坚实的基础。

#### 6.1.2 学习通用的物理规律与视觉-语言关联

在预训练阶段，VLA模型通过在海量合成数据上进行自监督或监督学习，能够自发地学习到许多通用的物理规律和视觉-语言关联。例如，模型可以学习到物体的恒常性（即使被遮挡，物体依然存在）、重力（物体会下落）、支撑关系（物体需要被支撑才能稳定）等基本物理概念。同时，模型也能建立起语言指令与视觉场景、机器人动作之间的复杂映射关系。例如，模型能够理解“红色的苹果”指的是哪个物体，以及“把它拿起来”对应的是怎样的动作序列。这种在预训练阶段建立起来的通用知识，是模型具备强大泛化能力的基础。

### 6.2 微调与适应阶段：提升特定任务性能

在预训练之后，VLA模型需要针对特定的机器人硬件和任务进行微调，以提升其在实际应用中的性能。合成数据在这一阶段同样发挥着重要作用。

#### 6.2.1 在特定任务的仿真环境中进行微调

对于特定的任务，可以在仿真环境中构建相应的场景，并生成大量的任务相关数据，用于对VLA模型进行微调。例如，如果目标是让机器人学会在工厂中装配零件，就可以在仿真中搭建一个虚拟的工厂环境，并生成大量的装配演示数据。在这种针对性的仿真环境中进行微调，可以高效地提升模型在该任务上的成功率。此外，还可以利用**强化学习（RL）** 在仿真环境中对模型进行进一步优化。由于仿真环境可以提供即时的、无成本的奖励反馈，因此可以高效地探索出更优的策略。

#### 6.2.2 结合少量真实数据进行协同训练

为了弥合Sim2Real Gap，微调阶段通常会结合使用合成数据和少量高质量的真实数据。这种**协同训练**的策略，旨在让模型既能学习到仿真环境中大规模数据的通用性，又能适应真实世界的细微差别。例如，可以先在仿真数据上进行初步微调，然后再用少量真实世界的演示数据进行精调。研究表明，这种混合训练的方式，往往比单独使用任何一种数据都能取得更好的效果 。**OpenVLA模型**就展示了其能够高效微调以适应新环境的能力，在多任务场景下，其任务成功率相比从零训练的模仿学习方法**提升了约20.4%** 。

### 6.3 持续学习与验证：闭环优化

具身智能模型的开发是一个持续迭代、不断优化的过程。合成数据在这一闭环优化流程中，提供了高效、安全的验证和探索平台。

#### 6.3.1 在仿真环境中快速验证新策略

当研究人员对VLA模型的架构或训练方法做出改进后，需要快速验证新策略的有效性。在真实世界中进行测试不仅成本高，而且周期长。而在仿真环境中，可以在几分钟内完成数千次的测试，从而快速评估新策略的性能。例如，理想汽车自研的世界模型，支持**每日30万公里以上**的测试里程，总计仿真数据已超**4000万公里**，每一个仿真样本都经由大模型评测、评分、反馈并强化训练，构成完整闭环 。这种高效的验证流程，极大地加速了模型的迭代速度。

#### 6.3.2 利用仿真数据进行安全、高效的失败案例探索

为了让模型学习到鲁棒的策略，需要让它经历大量的失败案例。在真实世界中，让机器人反复尝试可能导致损坏或危险的操作是不可行的。但在仿真环境中，则可以安全、高效地探索各种失败的可能性。例如，可以设置各种极端的初始条件，或者让模型在充满干扰和不确定性的环境中进行训练。通过分析这些失败案例，研究人员可以更好地理解模型的弱点，并针对性地进行改进。这种在仿真中进行“压力测试”的方式，是提升模型鲁棒性和安全性的重要手段。