# 扩增视觉-语言-动作模型（VLA）中合成数据生成方法的研究

## 1. 引言：合成数据在视觉-语言-动作模型中的重要性

### 1.1. 机器人学中视觉-语言-动作模型的兴起

视觉-语言-动作（VLA）模型代表了机器人学领域的一次变革性转变，旨在将视觉感知、自然语言理解和具身控制统一到一个单一的学习框架中 1。这些模型使机器人能够解释复杂的指令，感知其环境，并生成可执行的动作 3。VLA模型的核心目标是提升机器人的整体智能水平，使其能够通过整合多种模态自主解决一系列任务 3。这种范式旨在克服传统任务专用编程的局限性，因为传统方法在动态和非结构化环境中往往表现不佳 1。

### 1.2. 现实世界机器人学习中的数据瓶颈

开发强大的VLA模型面临的一个重大挑战是其对数据的巨大需求。训练机器人学中的深度网络通常需要数千个带标注的训练样本 6。然而，大规模收集现实世界中的人类演示数据本质上是耗时、资源密集且成本高昂的 7。机器人本体、传感器、执行器自由度以及控制模式的多样性进一步加剧了这种“数据孤岛”问题，阻碍了形成像其他AI领域那样大规模、连贯的数据集 10。此外，收集用于长时程和高精度任务的数据尤其具有挑战性且资源消耗巨大 9。

### 1.3. 合成数据作为可扩展解决方案

合成数据生成提供了一种引人注目的替代方案，以解决数据稀缺的挑战，有望实现可扩展性、多样性并降低成本 7。它使得在没有物理限制和手动劳动的情况下，能够创建大量的训练数据 13。生成式AI和自动化数据生成工具的最新进展进一步提升了合成数据在可扩展生成机器人行为数据集方面的潜力 7。

### 1.4. 报告结构与范围

本报告将系统地分析VLA模型合成数据生成领域的现状，将方法分为MimicGen系列、基于世界模型的生成方法以及通用数据扩增与多模态增强策略。对于每种方法，报告将详细阐述其技术基础、应用、定量性能、优势以及局限性。

## 2. MimicGen系列：从人类演示中进行可扩展模仿学习

MimicGen系列代表了一系列方法，它们利用少量人类演示来合成大规模数据集，主要用于机器人领域的模仿学习。这些方法侧重于将现有的人类提供的轨迹适应并转换到新的情境中。

### 2.1. MimicGen：演示适应的基础

MimicGen于2024年2月发布 14，是一个数据生成系统，通过将少量人类演示适应到新情境中来合成大规模数据集 14。其技术流程涉及将人类演示分解为以对象为中心的片段。对于一个具有不同对象姿态的新场景，MimicGen会选择一个人类演示，对其每个以对象为中心的片段进行空间变换，然后将它们拼接起来，并让机器人遵循这条新轨迹来收集新的演示 14。该系统利用模仿学习和行为克隆（BC）来训练策略。其动作空间由末端执行器增量姿态命令和夹持器开/关命令组成，将动作视为一系列目标姿态 14。MimicGen假设任务由已知序列的以对象为中心的子任务组成，其中每个子任务中的操作都相对于单个对象的坐标系 14。它使用线性插值（位置线性插值和旋转球面线性插值）来连接变换后的人类片段，并在当前末端执行器姿态和变换后片段的起点之间添加中间控制器姿态 14。

在数据规模和任务类型方面，MimicGen从大约200个人类演示中生成了超过50,000个演示，涵盖18项任务 14。它广泛适用于各种长时程和高精度任务，例如抓取放置、插入以及与关节对象交互 14。该方法能够为多样化的场景配置、同一类别中不同对象实例（如未见过的马克杯或马克杯清理任务中的多个马克杯），以及不同机器人手臂（如Sawyer、IIWA和UR5e手臂，源数据集在Panda手臂上收集）生成数据 14。它还与模拟器无关（支持robosuite和Factory），并在真实世界机器人手臂上进行了验证 14。

MimicGen的优势在于其可扩展性和成本效益，通过显著减少对大量人类演示收集的需求，为机器人学习提供了一种经济高效的方法 14。通过MimicGen生成的数据集训练的智能体表现出显著的性能提升，例如在Square任务中从11.3%提升到90.7%，在Threading任务中从19.3%提升到98.0% 14。即使源演示来自经验不足的人类操作员或不同的遥操作设备，通过MimicGen数据集训练的策略也能达到可比的性能 14。此外，它还有助于数据集的整理，以研究数据集构成如何影响学习策略的熟练度 14。

然而，MimicGen也存在一些局限性。它假设对任务中的以对象为中心的子任务有先验知识，并且该序列不会改变 14。在数据生成过程中，它需要每个子任务开始时有对象姿态估计 14。当前的系统假设每个子任务都只与一个对象相关，这限制了其在动作依赖于多个对象的任务中的适用性 14。MimicGen仅根据任务成功与否过滤数据，这可能导致数据集有偏差或产生不希望的伪影 14。所使用的线性插值方案不能保证无碰撞运动，并可能导致不自然的路径，从而可能损害智能体性能 14。尽管支持不同对象，但它仅在几何相似的、来自同一类别且规模相似的刚体对象上进行了演示，并假设规范坐标系对齐 14。MimicGen在准静态任务上进行了演示，以其当前形式不太可能适用于动态、非准静态任务，并且不考虑不同的对象动力学 14。当前的移动操作实现假设没有同时进行的基座和手臂运动，并且只是从参考片段复制移动基座动作，这对于多样化的环境布局可能不是最优的 14。MimicGen目前仅适用于单臂任务 14。尽管它显著减少了人类演示的规模，但仍依赖于少量（例如10-25个）人类演示作为基础 14。此外，性能增益在达到一定规模（例如1000个生成演示）后会呈现边际效益递减 14。模拟到现实的鸿沟依然存在，真实世界中的策略学习结果低于模拟，部分原因是真实世界中数据集规模较小以及为了安全而使用较大的插值片段，这使得智能体难以模仿 14。

MimicGen的核心优势在于能够从少量人类演示（例如200个演示）扩展到大量生成数据（例如50,000个）。这表明，即使是可扩展的合成数据生成方法也并非完全“脱离人类”。初始种子演示的质量和多样性仍然至关重要，这巧妙地将人类的努力从数量转移到初始质量和细致的任务分解上。数据生成中的边际效益递减也表明，仅仅生成更多数据并非总是最佳方案；源于初始人类种子的生成数据的多样性和相关性才是最重要的。

线性插值用于拼接片段是一种简单且计算效率高的方法，但它直接导致了“不自然的路径”和“无法保证无碰撞运动”。这揭示了一个基本权衡：生成简易性往往以物理合理性和安全性为代价，需要进一步的改进或更复杂的生成方法。线性插值是一种基本的几何操作，虽然在连接点时效率很高，但它忽略了机器人运动的底层物理和运动学。这直接导致了“不自然路径”和“无法保证无碰撞运动”的局限性。这是一个经典的工程权衡：更简单的算法（线性插值）更容易实现且计算速度更快，但在物理世界中牺牲了保真度和鲁棒性。这一局限性表明，在后续迭代或互补方法中需要更复杂的运动规划或动力学感知生成。

MimicGen明确指出，与模拟相比，“真实世界性能较低”，部分原因归结于真实机器人为了安全而采用的“较大插值片段”，以及智能体难以模仿这些片段。这表明，即使通过数据生成，真实世界部署的细微差别（例如安全约束、控制复杂性）也会引入模拟未能完全捕捉的新挑战。模拟到现实的鸿沟是一个众所周知的问题。MimicGen的发现提供了一个具体的因果联系：真实世界部署中对“较大插值片段以确保安全”的需求，使得生成的轨迹更难被机器人策略模仿。这表明，合成数据虽然源自真实人类演示，但可能仍包含伪影或简化（如线性插值），不足以在没有进一步适应或更复杂策略学习的情况下直接进行真实世界传输。这突出表明，这种鸿沟不仅是视觉上的，也是行为和运动学上的。

### 2.2. DexMimicGen：扩展到双臂灵巧操作

DexMimicGen（DexMG）于2025年ICRA会议上发布（arXiv预印本可能为2024年10月 15），是一个用于双臂灵巧机器人（包括人形机器人）的大规模自动化数据生成系统 16。它建立在MimicGen的核心思想之上，即利用少量人类演示，并通过物理模拟中的演示转换和重放来生成数据 16。其关键设计特点包括灵活的单臂子任务分割策略，允许独立手臂执行，同时适应协调阶段 16。它采用同步策略来精确对齐协调子任务期间的动作，并采用排序约束机制来强制执行顺序子任务的正确顺序 16。

在数据规模和任务类型方面，DexMimicGen从仅60个人类演示中，为双臂灵巧机器人自动生成了超过20,000个演示，涵盖9项任务 15。这些任务包括罐头分类、咖啡、倾倒、托盘提升、箱子清理、抽屉清理、穿线、运输和部件组装 15。它支持多种模拟器（Robosuite、BiGym）和真实世界部署 15，并能为不同的重置分布生成数据集 15。

DexMimicGen的优势在于能够为双臂灵巧操作（一个众所周知的数据收集难题领域）高效生成大规模数据集 15。它通过异步单臂执行、同步和顺序约束来处理多臂协调的挑战 16。其生成的数据集可以通过简单的行为克隆来产生在各种任务中表现出色的策略 15。它利用真实到模拟再到现实（Real2Sim2Real）的管道来训练用于真实世界的策略 15。此外，它通过使基于模拟的操作数据集更易于访问，促进了双臂操作算法的开发 16，并且在性能上比“演示-噪声”基线高出58%以上 16。

DexMimicGen继承了MimicGen的一些局限性，例如对假定子任务知识的依赖以及如果管理不当可能导致不自然路径的问题。尽管它显著减少了人类的努力（60个演示生成20,000个），但它仍然需要初始的人类演示 15。

DexMimicGen在双臂和灵巧操作方面的成功，与MimicGen的单臂焦点形成对比，突出了将合成数据生成扩展到更复杂机器人系统不仅需要更多数据，还需要捕捉增加的自由度和协调需求的架构适应（例如，单臂分割、同步策略）。这表明“MimicGen”范式是可扩展的，但并非轻而易举；它需要特定领域的增强。MimicGen的核心是以对象为中心的分割和空间变换。对于双臂任务，简单地独立应用于两只手臂可能会因协调需求而失败。DexMimicGen明确引入了“单臂子任务分割”、“同步策略”和“排序约束机制”。这是对双臂操作复杂性增加的直接架构响应，表明随着机器人任务变得更复杂，数据生成方法必须发展以捕捉这些复杂性，而不仅仅是扩展简单的轨迹。这意味着任务固有结构的更深层理解被编码到生成过程中。

DexMimicGen能够为“新的重置分布”生成数据，并且其性能优于“演示-噪声”基线（后者仅通过动作噪声重放源演示），这强调了结构化、语义信息丰富的数据扩增比简单随机化更重要。演示的智能转换带来了更好的泛化能力。演示-噪声基线是一种简单的数据扩增形式。相比之下，DexMimicGen使用以对象为中心的分割和空间变换。定量比较（DexMimicGen性能优于演示-噪声58%以上）清楚地表明，数据扩增的方式至关重要。随机噪声可能会增加数据量，但不一定能引入有意义的多样性或覆盖新的、与任务相关的配置。DexMimicGen的方法通过将演示适应到新的对象姿态和场景配置，明确旨在促进空间泛化，这是真实世界机器人学中的一项关键能力。

### 2.3. SkillMimicGen (SkillGen)：基于技能的演示生成

SkillMimicGen（SkillGen）于2024年CoRL会议上发布，并于2024年10月更新 9。它是一个自动化系统，用于从少量人类演示中生成演示数据集，侧重于操作技能 9。其技术流程包括将人类演示分割成独立的操作技能 9。然后，它将这些技能适应到新的情境中，并通过自由空间过渡和转移运动将它们拼接起来 9。这涉及通过运动规划连接技能片段 17。SkillGen提出了一个混合技能策略（HSP）框架，用于从SkillGen数据集中学习技能的启动、控制和终止组件 9。HSP智能体在闭环反应性技能和由运动规划执行的粗略过渡运动之间交替 17。

在数据规模和任务类型方面，SkillGen从仅60个人类演示中，在模拟中生成了超过24,000个演示，涵盖18种任务变体 9。对于特定任务，例如清理黄油垃圾任务，它从3个人类演示中生成了100个演示 17。任务包括清理黄油垃圾、咖啡、抓取放置牛奶、螺母组装、穿线、正方形和部件组装 17。它已应用于3个真实世界的操作任务，并在一个长时程组装任务上展示了零样本模拟到现实的迁移 9。它还可以在杂乱的工作空间中生成数据，即使存在人类演示中未见的大障碍物，并且可以为不同的机器人（从Panda到Sawyer）生成数据 17。

SkillGen的优势在于其极高的数据效率，只需少量人类演示（最少3个）即可开始 17。它显著提高了数据生成和策略学习性能，优于最先进的数据生成框架（例如，在咖啡任务中，其性能优于MimicGen：65% 对 14%） 9。它能够为大场景变化（包括杂乱和大型障碍物）生成数据 9。HSP框架将学习到的反应性技能与运动规划相结合，实现了稳健的序列化 17。它在螺母组装任务的真实世界中实现了35%的成功率，仅需1个模拟人类演示即可实现零样本模拟到现实迁移 17。此外，它还能够为不同机器人平台生成演示 17。

尽管SkillGen显著减少了人类输入，但它仍然依赖于初始人类演示来定义技能 9。人类演示中技能分割和标注的质量仍然至关重要。

SkillGen从原始演示片段（MimicGen）转向抽象的“技能”，并整合运动规划以实现“自由空间过渡”，是其重要的演进步骤。这种抽象化带来了更高的数据效率（3个人类演示生成100个）和对场景变化（杂乱、障碍物）的鲁棒性。这意味着通过将更高层次的语义理解（构成“技能”的要素）编码到生成过程中，系统可以更有效地泛化，并需要更少的人类输入。MimicGen的线性插值在拼接片段时存在局限性（不自然路径、碰撞）。SkillGen通过明确引入“自由空间过渡和转移运动”以及“运动规划”来解决这个问题。这是对MimicGen运动学局限性的直接解决方案。此外，通过分割成“技能”而非仅仅以对象为中心的片段，SkillGen捕获了更抽象、可重用的行为单元。这种更高层次的抽象与规划相结合，使系统能够适应更大的场景变化和未见的障碍物，从而实现更好的泛化，并需要显著更少的人类初始演示（3个对比MimicGen的200个）。这意味着智能的、分层的任务分解是实现合成数据生成中极致数据效率和鲁棒泛化的关键。

混合技能策略（HSP）框架在学习到的反应性技能和运动规划之间交替，展示了强大的协同作用。这种混合方法利用了两种范式的优势：学习策略用于复杂的、接触丰富的交互（技能），以及经典运动规划用于技能之间稳健、无碰撞的导航。这是生成物理上合理且安全的机器人轨迹的实用解决方案。纯模仿学习通常难以应对鲁棒性和对分布外状态的泛化，特别是对于长时程任务或需要在复杂环境中导航的任务。纯规划可能对精细操作来说过于脆弱或计算成本过高。HSP结合了这些：学习到的技能处理接触丰富阶段的“做什么”，而运动规划处理自由空间中的“如何到达”。这种混合方法减轻了各自的局限性，从而产生更鲁棒和更具泛化性的策略，这是SkillGen数据生成策略的直接优势。

### 2.4. DemoGen：实现空间泛化的全合成方法

DemoGen于2025年2月发布 18，是一种低成本、全合成的自动化演示生成方法 18。它通过将演示动作轨迹适应到新颖的对象配置来生成空间增强的演示，每个任务仅需一个人类收集的演示 18。视觉观测通过使用3D点云作为模态进行合成，这些点云可以通过3D编辑轻松操作以反映空间增强 18。在动作生成方面，DemoGen通过结合任务与运动规划（TAMP）技术（类似于SkillMimicGen）来发展MimicGen策略 18。它将轨迹分解为自由空间运动片段和对象上技能片段。技能片段被转换，运动片段通过运动规划重新规划 18。

在数据规模和任务类型方面，DemoGen在经验上显著提升了各种真实世界操作任务（8项任务）的策略性能，包括涉及可变形物体、灵巧手末端执行器和双臂平台的挑战性场景 18。它可以在单臂和双臂平台、使用平行夹持器和灵巧手末端执行器、以及从第三人称/自我中心视角进行部署 18。

DemoGen的优势在于其超低成本和高效率。生成一个演示轨迹仅需0.01秒的计算时间，使其效率极高 18。它对人类的依赖性极低，每个任务仅需一个人类收集的演示 18。通过泛化到未演示的配置，它显著增强了策略性能 18。它采用完全合成的管道，将生成的计划具体化为空间增强的演示，可直接用于策略训练 18。它能够处理复杂性，适用于可变形物体、灵巧手和双臂平台 18。此外，它还可以扩展以实现抗干扰和避障能力 18。

尽管DemoGen在生成方面是“完全合成的”，但它仍然依赖于单个初始人类演示 18。单个高质量人类演示的质量可能至关重要。对于某些VLA模型对输入的要求，其对点云作为视觉观测的依赖可能会限制视觉保真度，这取决于下游VLA模型对输入的要求。

DemoGen的核心创新在于其“完全合成”的视觉观测方法，通过利用3D点云和3D编辑，而不是依赖于模拟环境中的渲染。这与MimicGen/DexMimicGen在模拟中重放演示有着根本区别。通过直接操作3D点云，DemoGen可能绕过了光真实感渲染引擎的复杂性和计算开销，同时仍然能够实现空间增强。这为合成数据生成带来了新的效率途径，特别是对于可以直接处理3D表示的模型。传统的合成数据通常涉及从3D场景渲染逼真图像。这可能计算密集且需要高保真资产。DemoGen选择“点云作为观测模态”和“3D编辑”进行视觉合成是一个有意的设计决策。点云本质上是3D的，可以轻松转换。这种方法可能比完整的光真实感渲染显著更快、资源消耗更少，特别是如果下游VLA模型可以直接使用点云或可以快速转换为图像。这代表了一种通过在3D空间中直接操作视觉数据来优化速度和空间泛化的战略选择。

每个任务仅需“一个人类收集的演示”将“最小化人类努力”的概念推向了极致。这意味着该系统在从单个示例中提取核心“意图”或“技能”并进行泛化方面非常有效，而无需多个变体来学习任务空间。这可能对机器人的快速任务部署产生变革性影响。MimicGen需要200个演示，DexMimicGen需要60个，SkillMimicGen需要3个。DemoGen声称“一个人类收集的演示”是这一趋势的逻辑极限。这表明MimicGen的转换策略与TAMP（如SkillMimicGen中）的结合变得如此鲁棒，以至于它可以从单个实例中推断出底层任务结构并进行泛化。这对于实际机器人部署具有深远影响，因为它大大减少了新任务的人工标注和遥操作负担。

### 表1：MimicGen系列方法概述

| 方法名称                     | 发布时间                | 核心思想                  | 关键特点                                               | 典型数据规模（生成/源）           | 主要任务类型                 | 显著优势                                            | 主要局限性                                         | 人类演示依赖              |
| ------------------------ | ------------------- | --------------------- | -------------------------------------------------- | ---------------------- | ---------------------- | ----------------------------------------------- | --------------------------------------------- | ------------------- |
| MimicGen                 | 2024年2月             | 通过空间变换和拼接将人类演示适应到新情境。 | 以对象为中心的分割，线性插值，行为克隆。                               | 5万 / ~200              | 抓取放置，插入，关节对象，移动操作。     | 可扩展，成本效益高，提高智能体性能，对源质量鲁棒。                       | 假设子任务知识，需要对象姿态估计，朴素插值，对象迁移有限，无多臂支持，存在模拟到现实鸿沟。 | 少量源数据集（例如10-25个演示）。 |
| DexMimicGen              | 2024年10月（ICRA 2025） | 将MimicGen扩展到双臂和灵巧操作。  | 单臂子任务分割，同步，排序约束。                                   | 2万 / 60                | 双臂灵巧操作（例如，罐头分类，倾倒，组装）。 | 可扩展双臂数据，处理协调，性能优异的策略，真实世界迁移。                    | 继承MimicGen局限性，仍需人类演示。                         | 少量源数据集（例如60个演示）。    |
| SkillMimicGen (SkillGen) | 2024年10月（CoRL 2024） | 基于技能的演示生成与运动规划。       | 分割成操作技能，适应，通过自由空间过渡和运动规划拼接；混合技能策略（HSP）。            | 2.4万 / 60（或100 / 3）    | 通用操作，杂乱环境，长时程组装。       | 极高数据效率（3个演示），对场景变化鲁棒，整合学习与规划，零样本模拟到现实迁移，跨机器人迁移。 | 依赖初始人类演示进行技能定义。                               | 极少量源数据集（例如3-10个演示）。 |
| DemoGen                  | 2025年2月             | 从单个演示中进行全合成、低成本的空间扩增。 | 将动作轨迹适应到新颖配置；通过3D点云编辑合成视觉信息；MimicGen + TAMP进行动作生成。 | 未具体说明，但效率极高（0.01秒/演示）。 | 可变形物体，灵巧手，双臂，刚体抓取放置。   | 超低计算成本，最小人类依赖（1个演示），强大的空间泛化能力，处理复杂对象/平台。        | 依赖单个高质量人类演示，点云视觉可能限制某些VLA的保真度。                | 每个任务单个高质量人类演示。      |

## 3. 世界模型生成：利用生成式AI进行机器人数据生成

这一类别探讨了利用生成式AI（通常以世界模型的形式）创建合成数据的方法。与主要适应现有Go人类演示的MimicGen系列不同，这些方法可以生成全新的轨迹、场景或环境条件。

### 3.1. Real2Render2Real (R2R2R)：无需动力学模拟的光真实感渲染

Real2Render2Real (R2R2R) 于2025年5月14日发布 20，是一个可扩展的管道，用于生成数据以训练通用操作策略，无需动力学模拟或遥操作 20。其技术流程利用智能手机捕获的一个或多个对象扫描以及单个单目人类演示 20。它通过3D高斯泼溅（3DGS）重建详细的3D对象几何和外观，并使用GARField分割场景 20。利用4D可微分部件建模（4D-DPM）跟踪6自由度对象运动 20。在增强阶段，R2R2R随机化对象初始化，如果合适则插值对象运动轨迹（位置线性插值，旋转球面线性插值）。它采样抓取姿态，并使用PyRoki求解微分逆运动学问题，以计算平滑的关节空间轨迹 20。在并行渲染阶段，它使用IsaacLab纯粹作为渲染引擎来生成多样化的光真实感机器人执行，将对象设置为运动学而非动力学实体，从而避免了接触建模的复杂性 20。R2R2R生成的数据与π0-FAST和Diffusion Policy等策略兼容 20。

在数据规模和任务类型方面，R2R2R能够合成数千个物理上合理的、与机器人无关的演示 20。它在ABB YuMi IRB14000双臂机器人上评估了五项操作任务：单对象抓取、多对象交互、关节对象操作（水龙头、抽屉）和双臂协调 20。

R2R2R的优势在于其可扩展性和高效率，与人类遥操作相比，数据生成时间缩短了27倍（51个演示/分钟 对 1.7个演示/分钟） 20。它显著减少了人类的努力，仅需一个人类演示视频和智能手机扫描作为输入 20。它生成高视觉保真度的数据，可以训练跨不同机器人硬件的通用策略 20。通过在渲染过程中应用广泛的领域随机化（照明、相机外参、对象姿态），它减轻了模拟到现实的视觉鸿沟，提高了泛化能力 20。此外，它通过故意避免动力学模拟，解决了物理引擎中由于不完美资产而常见的穿透和不真实碰撞等问题 20。

然而，R2R2R也存在局限性。它放弃了物理模拟，限制了摩擦、柔顺性和力反馈的建模，这可能限制其在这些因素至关重要的任务中的适用性 20。轨迹生成是几何插值，不考虑环境上下文（例如干扰对象），可能导致不可行的路径 20。其操作任务范围集中在刚性和关节对象的抓取操作上；不支持可变形对象处理或非抓取策略 20。抓取生成仅限于对极采样，排除了多指手 20。在快速运动、严重遮挡或纹理差的表面下，跟踪鲁棒性较差 20。在人类演示依赖方面，它需要一个人类演示视频作为输入，以及初始10分钟的人工设置用于扫描和跟踪 20。

R2R2R明确决定放弃物理模拟（仅将IsaacLab用于渲染，将对象视为运动学实体）是一个深刻的发现。这表明，对于某些类别的操作任务，实现视觉真实感和运动学合理性比精确的物理模拟更关键且更易实现，而物理模拟本身往往会因不完美的资产而引入“模拟到现实”的挑战。这是一种反直觉但有效的扩展数据策略。传统的模拟到现实管道通常涉及物理引擎来生成动态交互。然而，R2R2R发现，由于“不完美或未精炼的模拟资产”，特别是夹持器与对象交互时，模拟动力学“经常偏离真实世界行为”。通过将渲染与物理分离，R2R2R简化了管道，避免了模拟到现实鸿沟的一个主要来源。这意味着对于主要依赖视觉和运动学线索的VLA模型，一个具有运动学合理运动（源自真实人类演示）的高度逼真视觉环境，可能比物理精确但视觉或交互有缺陷的模拟更有效。

R2R2R对“智能手机捕获的扫描”和“单个单目人类演示”的依赖，突显了高质量数据收集的日益普及。这使得初始数据输入更加民主化，摆脱了专业的遥操作设置，使管道更广泛地适用。高保真数据收集传统上需要昂贵且复杂的设备（例如，运动捕捉系统、专用遥操作设备）。R2R2R使用智能手机这种无处不在的设备，显著降低了收集基础人类演示的门槛。这对于将机器人学习扩展到大型研究实验室之外具有更广泛的意义，支持收集更多样化和“野外”的初始演示。

### 3.2. DreamGen / GR00T-Dreams：基于视频世界模型的神经轨迹

DreamGen于2025年5月20日发布 21，GR00T-Dreams于2025年7月16日发布 13，GR00T N1于2025年3月18日发布 10。DreamGen是一个四阶段管道，用于训练机器人策略，通过“神经轨迹”（由视频世界模型生成的合成机器人数据）实现泛化 21。GR00T-Dreams是基于DreamGen研究的工作流程 13。其技术流程包括：1. 

**微调视频世界模型**：图像到视频扩散模型在目标机器人上进行微调，以学习其动力学 21。2. 

**提示生成视频**：模型通过初始帧和语言指令进行提示，生成机器人视频，包括新环境中的新行为 21。3. 

**提取伪动作（神经轨迹）**：使用潜在动作模型（LAPA）或逆动力学模型（IDM）从这些视频中推断伪机器人动作 21。4. 

**策略学习**：这些“神经轨迹”（带有伪动作的视频）用于下游视觉运动策略学习 21。

GR00T N1是一个通用人形机器人的开放基础模型。它是一个具有双系统架构的VLA模型 10：系统2（视觉-语言模块）是一个预训练的VLM（NVIDIA Eagle-2 VLM，由SmolLM2 LLM和SigLIP-2图像编码器微调而来），用于解释环境和语言指令 10，以10Hz的频率运行 10。系统1（扩散Transformer模块）是DiT的变体，通过动作流匹配进行训练，生成高频（120Hz）闭环电机动作，并交叉注意力VLM输出 10。GR00T N1使用真实机器人轨迹、人类视频（网络数据）和合成生成数据集的异构混合进行训练 10。

在数据规模和任务类型方面，DreamGen使人形机器人能够在已见和未见环境中执行22种新行为，而仅需一个环境中单个抓取放置任务的遥操作数据 21。GR00T N1将内部收集的遥操作轨迹增加了10倍（从88小时增加到827小时） 10。在11小时内生成了780,000个模拟轨迹（相当于6,500小时的人类演示） 10。任务包括抓取放置、倾倒、打开/关闭关节对象、工具操作（锤击）、折叠毛巾、擦拭液体、舀M&M豆 21。在Fourier GR1、Franka Emika和SO-100机器人上进行了评估 22。GR00T N1在RoboCasa Kitchen（24项任务）、DexMimicGen跨本体套件（9项任务）、GR-1桌面任务（24项任务）和真实世界GR-1任务（对象到容器抓取放置、关节对象操作、工业对象操作）上进行了评估 10。

DreamGen的优势在于其零样本泛化能力，能够使用最少的真实数据解锁强大的行为和环境泛化能力，以适应新颖的行为和设置 21。它改变了机器人学习的范式，从扩展人类遥操作数据转变为通过世界模型扩展GPU计算 21。GR00T-Dreams蓝图使得GR00T N1.5的开发仅用了36小时，而手动收集则需要约3个月 13。GR00T N1有效整合了异构数据（网络数据、人类视频和合成数据），以提供广泛的先验知识和真实世界的基础 10。GR00T N1通过统一模型生成单臂、双臂和人形机器人本体的各种操作行为 10。它适用于接触丰富的任务、可变形物体和工具使用 21。

然而，DreamGen和GR00T-Dreams也存在局限性。GR00T N1-2B的训练需要约50,000 H100 GPU小时用于预训练；神经轨迹生成需要约105k L40 GPU小时 10。这表明计算成本很高。尽管旨在绕过模拟到现实的鸿沟，但在模拟液体和关节物体方面仍然存在一些固有的困难 10。在人类演示依赖方面，它仍然依赖于初始的人类收集轨迹来微调视频世界模型，以及作为GR00T N1训练金字塔中“真实世界数据”的组成部分，尽管规模显著减小 10。

DreamGen的核心思想是使用视频世界模型生成机器人视频，然后推断动作（“神经轨迹”），这代表了一种根本性的转变。它不再是适应现有的人类动作，而是基于学习到的动力学和语言提示生成全新的动作。这超越了单纯的数据扩增，实现了真正的行为合成，从而能够零样本泛化到原始人类演示中不存在的新行为和环境。MimicGen及其变体主要关注转换现有的人类演示。然而，DreamGen利用“图像到视频扩散模型”（视频世界模型）来创建新的机器人动作视频。这是一种生成式方法，意味着它可以合成从未被人类明确演示过，但由学习到的动力学和语言暗示的行为。这解锁了从单个抓取放置任务到22个新动词的“零样本行为泛化”，这是泛化能力上的质的飞跃。这种范式转变强调了生成式AI在创建真正新颖和多样化训练数据方面的力量，推动了“合成”在机器人学中的边界。

GR00T N1的双系统架构（系统2 VLM用于高层推理，系统1扩散Transformer用于低层动作）模仿了人类的认知过程。这表明，复杂的通用机器人智能受益于分层设计，其中“推理”模块处理感知和语言，然后指导单独的“动作”模块生成高频电机命令。这种架构通过将推理与直接电机控制分离，实现了更鲁棒和更具泛化性的VLA能力。单一的端到端VLA模型可能难以同时处理复杂的语言理解、视觉感知和高频电机控制。GR00T N1将较慢（10Hz）的VLM用于“解释”和较快（120Hz）的扩散Transformer用于“动作生成”的分离是一种务实的解决方案。这种模块化允许每个系统专门化，从而提高效率和潜在的鲁棒性。VLM可以执行更深层次的语义理解，然后指导更具反应性的动作生成。这意味着未来的通用机器人模型可能会采用类似的分层或模块化架构，以应对VLA任务的多方面需求。

GR00T N1的训练策略使用“异构混合”数据源，并将其组织成金字塔结构：底部是大量的网络数据和人类视频（提供广泛的先验知识），中间是合成数据，顶部是真实世界数据（提供基础）。这种多层方法突出了一个理解，即单一数据源不足以训练通用机器人。广泛、多样、通常保真度较低的数据提供了基础知识，而高保真、特定于本体的数据则确保了真实世界的基础。由于“数据孤岛”问题 10，没有一个大规模的通用机器人数据集。GR00T N1通过结合不同类型的数据来解决这个问题，每种数据都贡献不同的优势。网络数据和人类视频提供了大量的视觉和行为先验知识，但缺乏机器人特定的动作。合成数据提供了可扩展的机器人特定轨迹，但可能缺乏真实世界的细微差别。真实世界数据对于基础至关重要，但稀缺。金字塔结构意味着一种战略性的加权和协同训练方法，其中每一层都弥补了其他层的不足，从而产生更鲁棒和更具泛化性的模型。这是一种用于构建机器人基础模型的复杂数据策略。

### 3.3. Cosmos Transfer世界模型：多模态控制的条件世界生成

Cosmos-Transfer1于2025年4月3日发布 23，是一个基于扩散的条件世界生成模型，构建在Cosmos-Predict1之上 23。其技术流程通过新颖的ControlNet设计，向扩散Transformer（DiT）添加了多模态控制分支 23。每种模态都有一个独立的控制分支，这些分支独立训练并在推理期间融合 23。通过一个时空控制图实现空间和时间自适应控制，该图在特定位置和时间实例上对不同模态进行加权 23。这些图可以是手动设计、启发式规则派生或由单独的神经网络模块预测 23。它使用提示词升采样器（微调后的Pixtral-12B模型）将用户提示转换为与训练提示分布一致的更详细提示 23。

在数据规模和任务类型方面，Cosmos-Transfer1-7B是其首次实现。Cosmos-Transfer1-7B-Sample-AV是为自动驾驶任务微调的特殊版本 23。其控制模态包括：模糊视觉（Vis）、边缘（Canny）、深度（DepthAnything2）和分割（GroundingDino、SAM2） 23。对于自动驾驶，还包括高精地图（HDMap）和激光雷达（LiDAR） 23。它在TransferBench上进行了评估（涵盖机器人手臂操作、驾驶和自我中心日常生活的600个示例） 23。它能够从文本+视频输入生成704x1280分辨率的121帧视频 24。

Cosmos-Transfer1的优势在于其高度可控的生成能力，自适应多模态控制允许对场景元素和条件进行精细操作 23。它增强了模拟到现实的真实感，通过调整照明、颜色、纹理和细节，同时保持物理上合理的机器人动力学来改进模拟视频 23。它通过引入新颖的背景对象和为自动驾驶测试生成大量视觉变体来丰富场景复杂性 23。它实现了可扩展的推理，使用高端硬件（NVIDIA GB200 NVL72机架）实现了实时世界生成（5秒720p视频在4.2秒内生成） 23。它作为物理AI的通用构建模块，可用于自动驾驶应用和研究 24。

然而，Cosmos-Transfer1也存在局限性。它需要大量的计算资源才能实现实时生成（GB200 NVL72机架） 23。当深度/分割线索单独使用时，其稀疏性可能导致重建精度降低 23。用户提示可能与训练分布存在领域偏差，需要使用提示词升采样器进行缓解 23。该文章未提及对人类演示的任何直接依赖，而是侧重于从结构化输入进行条件生成 23。

Cosmos-Transfer1创新性地将ControlNet与多种模态（模糊、边缘、深度、分割、高精地图、激光雷达）和时空控制图结合使用，是条件生成的一种复杂方法。这使得对生成世界的控制达到了前所未有的水平，能够精确操纵场景属性和动力学。这超越了简单的随机化，实现了智能引导的生成，这对于有针对性的数据合成（例如自动驾驶的特定极端情况）至关重要。通用世界模型可以生成多样化的场景，但通常缺乏特定机器人训练场景所需的精细控制（例如，生成机器人在特定光照条件下抓取特定对象的数据）。Cosmos-Transfer1的ControlNet，结合通过时空控制图对模态进行自适应加权，直接解决了这个问题。通过允许不同的控制信号在不同时间影响生成视频的不同部分，它提供了高度的精确性，这对于通过生成与目标真实世界条件或特定挑战性场景紧密匹配的数据来弥合模拟到现实鸿沟具有极高价值。

明确提到使用“NVIDIA GB200 NVL72机架”实现“实时世界生成”，突显了基于世界模型的数据生成的可扩展性不仅是算法上的壮举，而且严重依赖于尖端硬件基础设施。这意味着最先进的合成数据生成能力将与巨大的计算投资紧密相关，预示着未来专业硬件将加速AI发展。实时生成复杂、光真实感视频是计算密集型的。NVIDIA明确指出硬件（GB200 NVL72机架）和加速比（从1个到64个GPU的40倍加速）的事实，强调了这种性能是通过紧密集成的硬件-软件解决方案实现的。这表明，对于旨在实现最先进合成数据生成的公司和研究人员来说，获取或投资此类高性能计算基础设施将是一个关键的差异化因素。这也意味着，合成数据生成的“成本”，虽然低于人类遥操作，但正在转向大规模计算资源。

### 3.4. SynGrasp-1B / GraspVLA：十亿级合成抓取数据

SynGrasp-1B / GraspVLA于2025年5月6日发布 11。GraspVLA是一个用于机器人抓取的基础模型，在SynGrasp-1B数据集上进行了预训练 11。SynGrasp-1B数据集的生成利用了物理模拟（MuJoCo），包含超过100,000个独特的对象模型（Objaverse的LVIS子集） 11。布局生成通过随机缩放和放置对象到桌面上，以创建多样化且物理上合理的场景 11。抓取合成和轨迹生成利用抓取合成算法（例如，用于稳定对趾抓取）和运动规划算法（CuRobo）生成无碰撞轨迹 11，并在MuJoCo物理模拟器中验证轨迹 11。视觉随机化和渲染使用Isaac Sim（光线追踪渲染）从两个视角渲染高质量RGB图像，并随机化照明、背景和相机设置 11。为提高效率，采用了缓存、异步数据写入、并行物理模拟和渲染等策略 11。

GraspVLA模型采用基于Transformer的架构，具有独立的视觉、语言和动作编码器 11。训练涉及跨模态的掩码建模；使用对比学习对齐视觉、语言和物理动作 12。它整合了自回归感知任务和基于流匹配的动作生成 11。渐进式动作生成（PAG）机制用于将互联网基础数据集的知识转移到抓取技能中，从而扩展到新类别 11。它使用InternLM2 1.8B作为VLM，DINO-v2和SigLIP作为视觉编码器 11。

在数据规模和任务类型方面，SynGrasp-1B包含十亿帧机器人抓取数据 11。它专注于杂乱场景中的通用灵巧抓取 12，涵盖了各种抓取场景：不同对象形状、大小、材料和环境条件 12。

SynGrasp-1B/GraspVLA的优势在于其前所未有的规模，十亿帧数据集能够训练高度可泛化的抓取基础模型 11。它实现了直接的模拟到现实迁移，在没有任何真实世界训练的情况下，在真实世界抓取任务中达到了85%的成功率，通过少量微调可提升至95% 12。它成功泛化到新颖的对象、光照、背景和抓取场景 12。它具有多模态能力，能够理解视觉线索和自然语言指令进行抓取 12。它提供了强大的闭环抓取策略，在常见和透明物体上优于传统抓取检测算法（AnyGrasp） 11。它还展示了强大的零样本/少样本适应能力 11。

然而，SynGrasp-1B / GraspVLA也存在局限性。在十亿级数据集上进行训练需要大量的计算资源，这可能限制了许多研究人员的访问 12。其任务范围主要集中在抓取操作上；在高度可变形物体或复杂操作（超出简单抓取）方面的性能需要进一步研究 12。当前的语言理解评估主要侧重于结构化命令，这意味着在更丰富的对话交互方面仍有提升空间 12。该过程完全是合成的，依赖于物理模拟和运动规划进行轨迹生成，不依赖人类演示 11。

SynGrasp-1B的“十亿帧”规模代表了“合成数据用于规模化”理念的最终实现。这表明，对于像抓取这样定义明确、基础性的机器人技能，完全在模拟中生成真正大规模、多样化的数据集，可以产生高性能模型，并具有显著的模拟到现实迁移能力，可能完全绕过对人类演示的需求。MimicGen系列减少了人类演示，而SynGrasp-1B则通过完全合成的方式生成所有数据，从而消除了人类演示。其庞大的规模（十亿帧）在机器人学习数据集中是前所未有的。这表明，对于可以在模拟中精确建模和随机化的任务（如刚体抓取），“大数据”方法（其中数量和多样性至关重要）可以带来强大的泛化能力和直接的模拟到现实迁移。这是完全自动化、模拟驱动数据生成潜力的有力证明。

GraspVLA作为“抓取基础模型”，表明了一种发展特定机器人核心能力专业基础模型的趋势。与单一的、包罗万象的VLA模型不同，这种方法表明，可以通过大规模、有针对性的合成数据集有效学习特定复杂技能（如灵巧抓取）的基础知识，然后将其作为更高级任务的坚实基础。通用VLA模型旨在实现广泛适用性。然而，某些机器人技能，如抓取，本质上是复杂且基础的。GraspVLA专注于这一特定技能，并在十亿帧数据上进行预训练，这表明“分而治之”的策略可能有效：利用大规模合成数据集为核心能力（如抓取、运动、操作原语）构建高度熟练的专业基础模型，然后将这些模型整合到更广泛的VLA架构中。这可能带来更鲁棒和高效的整体系统。

### 表2：世界模型生成方法概述

| 方法名称                     | 发布时间       | 核心思想                                | 关键特点                                                                     | 典型数据规模（生成/源）                                           | 主要任务类型                        | 显著优势                                                   | 主要局限性                                       | 人类演示依赖                                   |
| ------------------------ | ---------- | ----------------------------------- | ------------------------------------------------------------------------ | ------------------------------------------------------ | ----------------------------- | ------------------------------------------------------ | ------------------------------------------- | ---------------------------------------- |
| Real2Render2Real (R2R2R) | 2025年5月    | 通过重建人类视频中的3D信息，生成光真实感数据，_不进行_动力学模拟。 | 3DGS, GARField, 4D-DPM用于资产/轨迹提取；PyRoki用于逆运动学；IsaacLab用于渲染（运动学实体）。        | 数千 / 1个人类视频 + 智能手机扫描                                   | 单/多对象抓取，关节对象操作，双臂协调。          | 可扩展（比遥操作快27倍），人类努力少，光真实感，与机器人无关，绕过物理模拟问题。              | 放弃物理模拟（限制摩擦/柔顺性），无场景碰撞感知，限于抓取刚性/关节对象，跟踪鲁棒性。 | 单个人类视频，10分钟设置。                           |
| DreamGen / GR00T-Dreams  | 2025年5月/7月 | 从视频世界模型生成“神经轨迹”（合成机器人数据）。           | 4阶段管道：微调世界模型，提示生成视频，提取伪动作（LAPA/IDM），训练策略。GR00T N1：双系统（VLM + DiT）。        | 从1个抓取放置演示生成22种新行为，涵盖10个新环境；11小时内生成78万个模拟轨迹（相当于6500小时）。 | 通用操作，抓取放置，倾倒，工具使用，可变形物体，关节对象。 | 零样本行为/环境泛化，范式转变（扩展GPU计算），快速开发，异构数据整合，统一模型支持多种本体。       | 训练/生成计算成本高，液体/可变形物体模拟挑战。                    | 极少（1个抓取放置演示用于泛化），但GR00T N1使用混合真实/人类视频数据。 |
| Cosmos Transfer世界模型      | 2025年4月    | 多模态控制的条件世界生成。                       | 基于扩散（DiT, ControlNet）；时空控制图；模态：模糊，边缘，深度，分割，高精地图，激光雷达。                    | TransferBench上600个示例；121帧（704x1280）。                   | 机器人模拟到现实，自动驾驶数据增强。            | 高度可控，增强模拟到现实的真实感，多样化数据增强，可扩展推理（GB200实现实时）。             | 计算资源需求高，控制线索稀疏性可能限制保真度，提示领域偏差。              | 无（从结构化输入进行条件生成）。                         |
| SynGrasp-1B / GraspVLA   | 2025年5月    | 十亿级合成抓取数据用于抓取基础模型。                  | 基于物理模拟（MuJoCo, Isaac Sim）；抓取合成，运动规划（CuRobo）；基于Transformer的VLA模型；渐进式动作生成。 | 10亿帧 / 0个人类演示                                          | 杂乱场景中的通用灵巧抓取。                 | 前所未有的规模，直接模拟到现实（85%零样本），对新颖对象/场景的强大泛化能力，多模态，零样本/少样本适应。 | 训练计算资源高，任务范围限于抓取，语言理解限于结构化命令。               | 无（完全合成）。                                 |

## 4. 数据扩增与多模态增强策略

除了直接的合成数据生成，还有各种策略侧重于结合真实和合成数据，增强数据模态，或改进VLA模型架构，以弥合现实鸿沟并提高泛化能力。

### 4.1. 模拟与现实协同训练：弥合现实鸿沟

模拟与现实协同训练方法于2025年3月31日发布（v1），并于2025年4月2日更新（v2） 7。其核心技术流程涉及在模拟和真实世界数据集的混合上训练视觉运动策略 7。目标是最小化行为克隆动作损失，该损失是模拟数据和真实世界数据损失的加权和，由协同训练比例（α）控制 7。数据准备包括构建与真实世界语义相似的模拟“数字孪生”环境，并利用MimicGen和DexMimicGen等合成数据生成方法在这些数字孪生中倍增轨迹 7。此外，还使用“任务无关的先验模拟数据” 7。策略在混合数据上进行协同训练，然后直接部署到真实机器人上 7。该方法使用行为克隆和扩散策略 7。

在数据规模和任务类型方面，该研究在两种不同的机器人本体上进行了评估：Franka Emika Panda机器人（Panda厨房任务：台面到水槽抓取放置、台面到柜子抓取放置、关门）和Fourier GR-1人形机器人（人形桌面任务：杯子抓取放置、牛奶抓取放置、倾倒） 7。对于Panda机器人，每个真实世界任务使用50个人类演示；对于人形机器人，每个任务使用20个人类演示 7。任务无关的先验模拟数据包括Panda的6万条轨迹（RoboCasa），人形机器人的1万条轨迹 7。任务感知的数字孪生数据包括Panda的1万条合成轨迹（来自100个人类演示），人形机器人的1千条合成轨迹（来自10个人类演示），均使用MimicGen/DexMimicGen生成 7。

协同训练的优势在于其显著的性能提升，与仅使用真实数据的策略相比，它将真实世界任务性能平均提高了38% 7。即使模拟数据与真实世界数据存在显著差异，模拟数据也能提供实质性益处 7。它促进了对新颖对象和位置的泛化 7，并减少了对大规模真实世界数据收集的依赖 7。该方法提供了一个可操作的“秘籍”，为从业者提供了结合模拟和真实数据以实现卓越策略学习成果的实用见解和方法 7。

然而，协同训练也存在局限性。其任务范围主要集中在抓取放置任务；扩展到更复杂的任务是未来的工作 7。准确模拟可变形物体和液体仍然困难 7。尽管减少了真实世界人类数据的规模，但合成数据生成仍然依赖于初始的人类演示集 7。此外，相机对齐对于任务感知的数字孪生数据至关重要 7。

研究发现“即使模拟数据与真实世界数据存在显著差异，也能提供实质性益处”，这是一个关键的发现。它挑战了传统观念，即完美的模拟到现实对齐是成功迁移的先决条件。这意味着大规模、多样化合成数据的好处可能超过细致的领域随机化或动态对齐的成本，使得协同训练成为一种更实用、更鲁棒的策略。模拟到现实的鸿沟通常被视为一个需要精确对齐物理、视觉和控制的问题。然而，这篇论文明确测试了“任务无关的先验模拟数据”，这些数据与真实世界存在“显著差异”。这些数据仍然提供“实质性益处”（31.5%的改进）的事实表明，学习过程可以从不完美的模拟中提取可泛化的特征或行为。这意味着合成数据的庞大数量和多样性，即使不完美匹配，也可以作为强大的正则化器或提供广泛的先验知识，从而提高真实世界策略的鲁棒性，减少实现完美模拟保真度的负担。

研究观察到1:1的协同训练比例并非最优，而99%的模拟数据比例在某些任务中表现最佳，这突出表明真实数据和合成数据的最佳混合并非微不足道，而是一个关键的超参数。这表明未来的研究需要关注自适应或学习型协同训练策略，而非固定比例，以实现效益最大化。如果合成数据总是有益的，人们可能会认为越多越好，或者平衡混合是理想的。然而，发现高比例合成数据（99%）在某些任务中可能是最优的，这表明数据类型之间存在复杂的相互作用。这意味着真实数据可能作为关键的“基础”信号，而合成数据则提供了必要的规模和多样性。最佳平衡是任务相关的，需要仔细调整，这表明未来的VLA训练管道可能会纳入动态加权或课程学习以进行数据组合。

### 4.2. VLA-3D数据集：增强3D空间推理

VLA-3D数据集于2024年11月5日发布 26。该数据集旨在辅助导航和交互式智能体，侧重于与视角无关的空间关系 26。其技术流程包括3D扫描处理、场景图生成和语言生成。3D扫描处理阶段从各种真实世界3D数据集（ScanNet、Matterport3D、HM3D、3RScan、ARKitScenes、Unity）获取场景级点云 26。它识别区域和对象，映射语义类别，提取主要颜色，并生成可遍历的自由空间 26。场景图生成阶段使用基于偏航对象边界框的启发式方法计算八种语义空间关系（上方、下方、最近、最远、之间、附近、内部、上方） 26。语言生成阶段使用基于模板的方法和同义词，根据计算出的场景图合成生成指代性语言语句 26。这些语句是与视角无关、无歧义且最小化的 26。该数据集还对MVT和3D-VisTA等现有模型进行了基准测试 26。

在数据规模和任务类型方面，VLA-3D数据集包含超过11.5K个扫描的3D室内房间、23.5M个启发式生成的对象间语义关系以及9.7M个合成生成的指代性语句 26。它包括7635个场景，包含超过11.5K个区域和28.6万个来自477个独特类别的对象 26。该数据集侧重于指代性对象定位和使用自然语言指令进行室内导航 26。

VLA-3D数据集的优势在于其大规模的3D视觉-语言数据，解决了此类数据相对于2D数据的稀缺性 26。通过合成启发式生成，它显著降低了标注成本 26。它侧重于空间推理，旨在通过强调与视角无关的空间关系和可遍历的自由空间来辅助下游导航任务和交互式智能体 26。其较低的基线性能表明它是一个有效的基准，可以推动3D视觉-语言方法的进步 26。

然而，VLA-3D数据集也存在局限性。现有最先进模型性能较低（22.5-28.9%的准确率），表明在向复杂真实世界场景进行跨领域泛化方面存在显著挑战 26。大多数基线模型难以处理开放词汇对象名称 26。当前模型无法处理语言输入中存在错误的情况 26。尽管是自动化生成，但基于模板的语言生成可能缺乏人类生成语言的全部细微差别和可变性。标注成本方面，由于自动化生成，成本显著降低 26。

VLA-3D数据集尽管规模庞大，但揭示了当前VLA模型在3D空间推理任务上表现出“较差的跨领域泛化能力”和“远低于”2D基准的性能。这突出表明了一个关键的差距：虽然VLA模型擅长利用2D视觉-语言预训练，但它们对RGB图像的依赖限制了对真实世界交互至关重要的空间推理能力，并且仅仅提供更多3D数据并不能自动解决这个问题。VLA模型通常建立在大型2D视觉-语言模型之上。然而，机器人任务本质上发生在3D空间中，需要理解空间关系（例如，“附近”、“上方”、“之间”）。VLA-3D数据集明确旨在测试这一点。尽管该数据集规模庞大，但最先进模型在其中表现出的“低基线性能”表明，挑战不仅在于数据量，还在于VLA模型如何处理和推理3D空间信息。这意味着需要架构创新或训练方法，明确整合超出2D图像隐含学习的3D理解。

VLA-3D使用“基于模板的生成”来生成语言，确保语句“与视角无关、无歧义且最小化”。虽然这降低了标注成本，但可能会牺牲自然人类语言的丰富性和可变性，因为自然语言通常包含隐含的上下文、回指，甚至微小错误。这表明在高度自动化、成本高效的数据生成与模型理解真正自然、无约束人类语言的愿望之间存在张力。自动化语言生成是高效的。然而，人类语言是复杂的，并且经常偏离严格的模板。VLA-3D专注于“无歧义”和“最小化”的语句，虽然有利于清晰的基础，但可能无法完全让VLA模型为真实人类指令的混乱做好准备。这表明，虽然合成数据可以扩展，但合成语言的质量（就其自然度和可变性而言）可能成为真正鲁棒的人机交互的瓶颈。未来的工作可能需要探索更先进的生成式语言模型来创建更自然的指令。

### 4.3. OneTwoVLA：统一推理与行动

OneTwoVLA于2025年5月发布 27，是一个能够同时执行行动（系统一）和推理（系统二）的统一VLA模型 27。其技术流程在于它在关键时刻（例如，完成子任务、检测错误、需要人类输入）自适应地在显式自然语言推理和基于最新推理生成动作之间切换 27。它采用可扩展的管道来合成以具身推理为中心的视觉-语言数据，用于与机器人数据进行协同训练 27。

在数据规模和任务类型方面，OneTwoVLA在“单环境”（4个对象，200个抓取演示）和“开放世界”（16个多样化的野外环境，933个演示，180个不同的家居物品）设置下进行了评估 27。它展示了在长时程任务规划、错误检测与恢复、自然人机交互和可泛化视觉定位方面的能力 27。

OneTwoVLA的优势在于其集成的推理与行动能力，允许根据反馈动态调整任务计划 27。它通过准确解释空间关系、对象属性和语义特征，实现了显著更高的视觉定位成功率（例如，78% 对 基线的5%） 27。它支持错误检测与恢复 27，促进了自然人机交互 27，并提高了对未见对象和环境的泛化能力 27。

然而，OneTwoVLA也存在局限性。它需要一个可扩展的管道来合成以推理为中心的数据，这暗示了数据生成的复杂性。显式推理可能会引入延迟，尽管自适应切换旨在缓解这一点。尽管有“以推理为中心”的数据，但对真正新颖、复杂场景的泛化可能仍然具有挑战性。

OneTwoVLA将显式“推理”（系统二）与“行动”（系统一）相结合，是VLA模型发展中的一个关键演变。它超越了简单地模仿观察到的动作，使机器人能够理解其环境、规划并使用自然语言从错误中恢复。这表明VLA模型的下一个前沿不仅是生成更多数据，还在于生成能够促进更深层认知能力的数据。传统的VLA模型通常专注于从感知+语言到动作的直接映射（行为克隆）。OneTwoVLA引入了一个独立的“推理”模块，该模块生成自然语言输出（场景描述、任务计划、错误检测）。这意味着对于长时程、复杂或交互式任务，机器人需要做的不仅仅是执行；它需要理解原因以及如何调整。以“推理为中心”的合成数据旨在训练这种能力，这表明机器人行为正朝着更智能、更不易出错的方向发展。

开发一个“可扩展的管道，用于合成以具身推理为中心的视觉-语言数据”意味着合成数据不仅用于扩展运动技能，还用于训练更高层次的认知功能，如规划、错误检测和视觉定位。这扩展了合成数据生成的范围，超越了单纯的动作轨迹，包括支持复杂推理的结构化语言和感知数据。如果VLA模型要进行推理，它需要明确链接观测、语言和推理输出的训练数据。手动标注复杂场景的此类数据将成本高昂。创建一个“以推理为中心”的合成数据管道直接解决了这个问题。这表明合成数据可以被设计为针对VLA模型中的特定认知缺陷，使其能够学习复杂任务和人机交互所需的更抽象和鲁棒的表示。

### 4.4. Interleave-VLA：图像-文本指令理解

Interleave-VLA于2025年5月5日发布 29，是一个能够理解交错图像-文本指令并直接生成连续动作序列的框架 29。其技术流程采用基于Transformer的架构，以交错方式处理视觉和文本输入，分别编码并通过交叉注意力进行组合 29。一个自动化管道将真实世界数据集（Open X-Embodiment）中的纯文本指令转换为交错图像-文本指令 30。训练涉及对演示数据集的监督学习和通过机器人交互进行的强化学习 29。

在数据规模和任务类型方面，Interleave-VLA创建了“Open Interleaved X-Embodiment”数据集，包含210,000个片段 30 或超过100,000个演示 29。它专注于通用机器人操作任务，特别是那些需要复杂、多模态指令的任务 29。它能够以零样本方式处理多样化的用户提供图像指令，包括手绘草图 30。

Interleave-VLA的优势在于其增强的指令理解能力，与纯文本指令相比，任务完成率提高了15% 29。它实现了2-3倍的域外泛化能力，能够处理未见对象 30。它支持灵活的任务接口，包括手绘草图等多样化用户输入 30。它有效利用异构数据，包括来自互联网的各种指令图像，以实现扩展 30。自动化数据转换减少了手动标注成本，通过转换现有纯文本数据集实现 30。

然而，Interleave-VLA也存在局限性。它需要大量带有配对图像-文本指令的训练数据 29。它可能难以处理需要长期规划的任务 29。性能严重依赖于视觉输入的质量 29。尽管自动化管道降低了成本，但纯文本数据集的初始创建和交错格式的复杂性意味着大量的数据整理工作 29。

Interleave-VLA的核心贡献在于证明了图像和文本按顺序组合的指令比纯文本指令能显著提高机器人性能和泛化能力。这表明对于复杂的多步骤任务，机器人从“带图片的食谱”方法中受益匪浅，这反映了人类学习复杂过程的方式。这对于未来人机界面的设计具有重要意义。人类自然地使用视觉线索和口头指令（例如，“把杯子放在这里”并伴随指向手势）。传统的VLA模型通常依赖于文本或单张图片。Interleave-VLA将“交错”指令格式正式化，这更类似于人类进行程序性任务的交流方式。任务完成率提高15%和对未见对象泛化能力提高2-3倍直接证明了这种丰富指令格式与机器人能力提升之间的因果关系。这意味着设计合成数据以包含此类多模态、顺序指令是增强VLA模型学习的强大方式。

开发一个“自动化管道，将纯文本指令……转换为交错图像-文本指令”是实现这种方法规模化的关键。这突出了一种利用现有大型数据集的策略，通过将其转换为更有效的多模态格式，而不是从头开始收集全新的交错数据。手动创建210,000个带有同步图像-文本对的片段将成本高昂。自动化管道对于Interleave-VLA的可行性至关重要。这表明创新的数据处理和转换技术可以释放现有单模态数据集在多模态VLA训练中的潜力，从而降低新模态的有效标注成本。这是未来数据扩展工作的一个宝贵经验：寻找智能地转换和丰富现有数据的方法。

### 4.5. QUAR-VLA / QUARD：四足机器人VLA

QUAR-VLA于2024年10月发布 2，QUART于2025年3月发布 31，MoRE于2025年3月22日发布 2。QUAR-VLA是四足机器人的一种新颖范式，它整合了视觉信息和指令以生成可执行动作 4。QUART（四足机器人Transformer）是一个VLA模型，它以机器人第一视角相机图像和自然语言指令作为输入，生成控制命令（基础速度、姿态、步态参数） 4。它利用预训练的大规模VLM，并在QUARD数据集上进行微调 4。MoRE（机器人专家混合）是一个新颖的四足VLA模型，它将多个低秩适应（LoRA）模块作为不同的专家集成到密集的MLLM中 2。它接收RGB图像和语言指令，直接输出控制命令 32。它使用基于强化学习的训练目标，用于使用混合质量数据进行微调 2。QUART-Online解决了MLLM在QUAR-VLA任务中固有的推理延迟挑战 31。它引入了动作块离散化（ACD）来压缩动作表示空间，将连续值映射到离散向量 31。

在数据规模和任务类型方面，QUARD（四足机器人数据集）是一个大规模多任务数据集，包括感知、导航和全身操作任务 4。QUART训练使用25.9万个模拟片段和3千个真实世界片段 4。QUARD离线数据集包括1,822,405个来自人类演示的视觉-语言-动作集（6种任务类型：区分、前往、穿过、避开、卸载、爬行）和440,732个来自QUARD-Auto的集（4种任务类型） 2。MoRE在六种不同的技能上进行了评估 2。

QUAR-VLA的优势在于其通用四足机器人控制能力，使四足机器人能够自主导航和执行各种任务，并整合了感知、规划和决策 4。它展现了理解新颖命令、泛化到未见对象和推理等涌现能力 31。MoRE通过有效学习自动收集的混合质量数据，提高了数据效率 2。QUART-Online实现了50Hz的实时推理，将各种任务的成功率提高了65% 31。MoRE优于基线模型，并在分布外场景中展现出卓越的泛化能力 2。

然而，QUAR-VLA也存在局限性。四足机器人执行多样化任务的大规模数据集稀缺 4。MLLM可能存在固有的推理延迟挑战 31。VLA模型可能容易受到对抗性攻击，任务成功率显著下降 2。对RGB图像的依赖限制了对真实世界交互至关重要的空间推理能力 2。弥合模拟到现实的鸿沟仍然是一个挑战 31。

QUAR-VLA专注于四足机器人以及“前往”、“穿过”和“爬行”等任务，这标志着VLA范式超越传统机器人操作的关键扩展。这表明VLA模型在具身AI方面的多功能性，涵盖了动态环境中的运动、导航和全身控制，开辟了新的应用领域。大多数VLA研究都集中在机器人手臂和操作任务上。QUAR-VLA明确将VLA框架应用于四足机器人，这主要关注运动和复杂地形导航。QUARD中包含“前往”和“爬行”等任务，突出表明VLA概念适用于更广泛的机器人行为，而不仅仅是对象交互。这意味着视觉-语言定位和动作生成原则可以泛化到不同的机器人本体和任务类型，为真正的通用机器人铺平道路。

QUART-Online引入动作块离散化（ACD）以实现“无延迟”实时推理（50Hz），对于VLA模型在四足机器人等动态机器人上的实际部署至关重要。这突出表明，计算效率和实时性能不仅是理想功能，而且是具身AI的基本要求，尤其是在处理高频控制循环时。大型多模态语言模型（MLLM）功能强大，但通常计算量大，导致推理延迟。对于四足机器人，实时反应控制对于稳定的运动和交互至关重要。QUART-Online通过压缩动作空间直接解决了这个问题，允许MLLM以控制器的频率输出动作。这表明，专注于效率的架构和算法创新对于将大型VLA模型的强大功能从理论承诺转化为实际、真实世界的部署是必要的，特别是对于在动态环境中运行的机器人。

MoRE使用强化学习对大规模VLA模型进行微调，并使用“大量混合质量数据”（包括来自QUARD-Auto的次优数据），这是一种重要的策略。这表明，即使是不完美的、自动收集的数据，如果与强化学习相结合，也能有效利用，因为强化学习可以从多样化的经验中学习，包括失败。这拓宽了可用合成数据的范围。收集纯粹的“最优”人类演示既具有挑战性又成本高昂。MoRE能够从“混合质量数据”（包括“次优数据”）中学习，这是一个实际优势。强化学习本质上可以从试错和多样化经验中学习，使其适用于利用此类数据。这意味着未来的合成数据生成管道可能不需要追求每个生成轨迹的完美最优性；相反，生成大量多样化数据，即使其中一些是次优的，如果训练框架（如强化学习）能够有效学习，也可能是有益的。

### 4.6. ReBot：从现实到模拟再到现实的机器人视频合成

ReBot于2025年3月15日发布 33，是一种新颖的现实到模拟再到现实（R2S2R）方法，用于扩展真实机器人数据集并将VLA模型适应到目标领域 33。其技术流程包括：1. 

**现实到模拟（R2S）**：ReBot在模拟中重放真实世界机器人轨迹，以多样化操作对象 33。2. 

**模拟到现实（S2R）**：它将模拟运动与修复后的真实世界背景整合，以合成物理上真实且时间一致的机器人视频 33。ReBot采用基于Transformer的架构，处理视频帧以预测适当的机器人动作 34。该模型在合成数据集上进行训练，然后在真实世界任务上进行评估 34。该架构结合了视觉对象检测和跟踪能力，以确保在不同条件下实现稳健性能 34。

在数据规模和任务类型方面，ReBot能够生成多样化、光真实感的机器人视频，展示各种任务、环境和对象 34。它在SimplerEnv中使用WidowX机器人，在真实世界中使用Franka机器人进行了评估 33。任务包括抓取放置和通用操作 34。

ReBot的优势在于它能够扩展机器人学习，通过弥合模拟与现实之间的鸿沟 34。它结合了真实数据的好处（最小化模拟到现实的鸿沟）和模拟的可扩展性 33。它能够将预训练的VLA模型泛化到目标领域，并具有完全自动化的数据管道 33。在SimplerEnv中，ReBot将Octo的域内性能提高了7.2%，OpenVLA提高了21.8%，域外泛化分别提高了19.9%和9.4% 33。在真实世界评估中，ReBot将Franka机器人上Octo的成功率提高了17%，OpenVLA提高了20% 33。它在真实机器人任务上达到了89.4%的成功率，主要使用合成数据训练的策略 34。

然而，ReBot也存在局限性。它仍然需要一些初始的真实机器人演示来启动合成数据生成过程 34。其泛化能力在更复杂、非结构化且具有不可预测元素的真实世界机器人应用中仍不确定 34。

ReBot的R2S2R管道，通过在模拟中重放真实世界轨迹并将其与真实背景融合，提供了一种弥合现实鸿沟的循环方法。这使得VLA模型能够适应目标领域，同时结合了真实数据的优势和模拟的可扩展性。传统的模拟到现实方法通常面临物理模拟的挑战。ReBot的“现实到模拟再到现实”方法通过利用真实世界的初始演示来引导模拟，然后将模拟结果重新整合到真实世界的视觉背景中，从而解决了这个问题。这种循环方法确保了合成数据的物理合理性和视觉真实感，同时避免了从头开始构建完美模拟的复杂性。它有效地利用了真实数据的“锚定”作用，同时享受了模拟在多样化和扩展方面的优势，从而为VLA模型的实际部署提供了更稳健的策略。

ReBot通过将模拟运动与修复后的真实世界背景整合来合成视频，突出了背景与前景分离的重要性。这种方法允许在保持真实世界背景不变的情况下，多样化前景中的机器人操作，从而生成大量具有高视觉保真度和物理合理性的训练数据。在许多合成数据生成方法中，改变整个场景（包括背景）可能会引入不必要的复杂性或不真实感。ReBot通过“修复真实世界背景”并将其与模拟的机器人运动相结合，实现了前景（机器人和操作对象）与背景的分离。这意味着可以独立地对机器人动作和对象配置进行大规模随机化，而无需担心背景的物理一致性或真实感，因为背景直接来自真实世界。这种模块化的数据生成方法提高了效率和数据质量。

## 5. 讨论与建议

### 5.1. 综合分析：合成数据生成方法的演进与趋势

对视觉-语言-动作（VLA）模型中合成数据生成方法的研究揭示了一个清晰的演进路径，旨在解决现实世界机器人学习中固有的数据瓶颈。最初，MimicGen系列的方法通过将少量人类演示适应到新情境中，实现了可扩展的模仿学习。这些方法从单臂操作（MimicGen）发展到双臂灵巧操作（DexMimicGen），并通过引入技能抽象和运动规划（SkillMimicGen）显著提高了数据效率和泛化能力。DemoGen将这一趋势推向极致，通过3D点云编辑实现完全合成的视觉，并仅需一个人类演示即可生成数据。

与此同时，基于世界模型的生成方法则代表了另一种范式。Real2Render2Real通过避免复杂的物理模拟，专注于光真实感渲染和运动学合理性，利用智能手机捕获的数据生成数千个演示。DreamGen和GR00T-Dreams利用视频世界模型生成“神经轨迹”，实现了对新行为和环境的零样本泛化，将机器人学习的重点从扩展人类遥操作数据转移到扩展GPU计算。Cosmos Transfer世界模型通过多模态控制和先进的生成式AI，实现了高度可控的条件世界生成，特别适用于模拟到现实的迁移和自动驾驶数据增强。SynGrasp-1B和GraspVLA则通过生成十亿帧的合成抓取数据，展示了在特定机器人技能上实现大规模“大数据”方法，从而实现了强大的零样本模拟到现实迁移，甚至可能完全摆脱人类演示依赖。

此外，数据扩增和多模态增强策略通过协同训练（模拟与现实数据的混合）、开发3D空间推理数据集（VLA-3D）、统一推理与行动（OneTwoVLA）、增强图像-文本指令理解（Interleave-VLA）以及将VLA范式扩展到四足机器人（QUAR-VLA/QUARD）等方式，进一步弥合了现实鸿沟并提高了泛化能力。这些方法共同描绘了一个持续推动减少人类依赖、提高泛化能力、增强真实感和物理合理性的发展蓝图。然而，这种进步也伴随着对计算资源需求的不断增长，尤其是在采用先进生成模型时，这表明了从扩展人类努力到扩展计算资源的根本性转变。

### 5.2. 挑战与未来方向

尽管合成数据生成取得了显著进展，但仍存在一些挑战需要克服：

- **现实鸿沟的持续挑战：** 尽管R2R2R等方法试图绕过物理模拟，但对于涉及复杂物理（如可变形物体、液体）和动态交互的任务，模拟到现实的鸿沟仍然存在。ReBot的循环方法虽然有效，但仍需初始真实数据。
    
- **数据质量与多样性：** 模板化语言生成（如VLA-3D）可能缺乏自然语言的细微差别和复杂性。确保合成数据真正覆盖现实世界的复杂性和多样性，包括极端情况和异常行为，仍然是一个挑战。
    
- **计算资源需求：** DreamGen、GR00T N1和Cosmos Transfer等先进的生成模型对GPU资源的需求巨大，其训练和生成过程需要大规模计算集群。这可能限制了这些方法的广泛应用和小型研究团队的访问。
    
- **高阶认知能力的训练：** 训练VLA模型以实现推理、规划、错误检测和恢复等高阶认知能力（如OneTwoVLA所示）需要更复杂的合成数据生成策略，这些策略不仅要生成动作，还要生成支持这些认知过程的结构化语言和感知数据。
    
- **评估基准的完善：** 现有基准可能无法完全捕捉VLA模型在3D空间推理、多模态指令理解和实时性能方面的细微差别。需要更全面的评估协议来准确衡量这些模型的真实世界能力。
    

### 5.3. 建议

基于对现有合成数据生成方法及其演进的分析，提出以下建议：

- **投资混合数据策略：** 鉴于模拟数据即使与真实世界数据存在差异也能提供实质性益处，建议采用协同训练策略，结合少量高质量真实数据与大规模合成数据。这能有效弥合现实鸿沟，并利用合成数据的规模优势。
    
- **优先发展具身智能的特定基础模型：** 针对抓取、运动、导航等核心机器人技能，鼓励开发专门的合成数据生成方法和基础模型（如SynGrasp-1B/GraspVLA）。这些专业模型可以作为更广泛VLA架构的强大基石，提高整体系统的鲁棒性和效率。
    
- **探索更智能的合成数据生成：** 结合生成式AI（如视频世界模型）、任务与运动规划（TAMP）和3D编辑技术，生成更具物理一致性、语义丰富性且能促进高阶认知能力的数据。应关注如何从极少量人类演示中提取核心技能和意图，并进行大规模泛化。
    
- **优化计算效率：** 鉴于先进VLA模型对计算资源的巨大需求，应持续研究并实施低延迟的推理策略（如QUART-Online的动作块离散化），以确保VLA模型能够在真实世界的动态环境中进行实时部署。
    
- **推动多模态指令范式：** 鼓励开发和利用图像-文本交错指令（如Interleave-VLA），因为这已被证明能显著提高机器人对复杂任务的理解和泛化能力。未来的合成数据生成应考虑如何高效生成此类多模态、顺序指令数据。