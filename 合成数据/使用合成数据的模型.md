
| 模型名称                   | 发布时间              | 是否使用合成数据训练 | 合成数据使用方式                                                                             | 真实演示依赖          | 备注与引用资料                                                                                                                                                                                                                                                                                                                                                                                                 |
|------------------------|------------------|-----------|------------------------------------------------------------------------------------|---------------|-------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------|
| **GraspVLA**           | 2025‑05          | ✅ 是       | SynGrasp‑1B：十亿帧 MuJoCo/Isaac Sim 仿真生成的抓取轨迹                                           | 少量真实用于微调       | 合成占主导，适配零样本泛化 ([xiaoxiao0406.github.io](https://xiaoxiao0406.github.io/vqvla.github.io/?utm_source=chatgpt.com "VQ-VLA: Improving Vision-Language-Action Models via ..."), [arXiv](https://arxiv.org/abs/2505.03233?utm_source=chatgpt.com "GraspVLA: a Grasping Foundation Model Pre-trained on Billion-scale Synthetic Action Data"))                                                                 |
| **GR00T N1 / N1.5**    | 2025‑03 / 2025‑06| ✅ 是       | Omniverse / Cosmos 合成轨迹 + 人类视频与真实机器人演示；N1.5 以 DreamGen 管道快速生成大量神经轨迹合成数据              | 极少真实示范        | 合成与真实混合推动泛化能力 ([phospho](https://blog.phospho.ai/dissecting-groot-n1-a-foundation-model-for-generalist-humanoid-robots/?utm_source=chatgpt.com "Dissecting GROOT N1: A Foundation Model for Generalist ..."), [phospho](https://blog.phospho.ai/d-groot-n1-5-a-foundation-model-for-generalist-humanoid-robots/?utm_source=chatgpt.com "Deep dive into GROOT N1.5: A Foundation Model for ..."))        |
| **ReBot‑增强 OpenVLA**   | 2025‑03          | ✅ 是       | ReBot “real-to-sim-to-real” pipeline：真实轨迹 replay 仿真 + 合成视频训练增强                       | 基于真实轨迹增强       | 提升实际任务成功率 20%以上 ([phospho](https://blog.phospho.ai/dissecting-groot-n1-a-foundation-model-for-generalist-humanoid-robots/?utm_source=chatgpt.com "Dissecting GROOT N1: A Foundation Model for Generalist ..."))                                                                                                                                                                                         |
| **ReBot‑增强 Octo**      | 2025‑03          | ✅ 是       | 同上：将真实轨迹仿真 replay 并融合真实背景生成合成训练视频                                                    | 同上             | Octo 真实任务性能提升约 17% ([phospho](https://blog.phospho.ai/dissecting-groot-n1-a-foundation-model-for-generalist-humanoid-robots/?utm_source=chatgpt.com "Dissecting GROOT N1: A Foundation Model for Generalist ..."))                                                                                                                                                                                     |
| **Helix (Figure AI)**  | 2025‑02          | ✅ 是       | S1 系统使用约 ~500 小时仿真生成场景训练，结合域随机化等合成技术，大比例合成数据驱动训练                                     | 少量真实遥控演示       | 仿真占训练主导 ~78% ([维基百科](https://en.wikipedia.org/wiki/Vision-language-action_model?utm_source=chatgpt.com "Vision-language-action model"))                                                                                                                                                                                                                                                                 |
| **Psi‑R1 (PsiBot)**    | 2025‑04          | ✅ 是       | 强化学习在仿真环境中执行麻将等长周期复杂任务，训练策略囊括 synthetic data                                         | 模拟为主           | 公司公开称“一切能力基于仿真 synthetic data” ([AIModels](https://www.aimodels.fyi/papers/arxiv/vq-vla-improving-vision-language-action-models?utm_source=chatgpt.com "VQ-VLA: Improving Vision-Language-Action Models via ..."))                                                                                                                                                                                      |
| **Era‑42 (Robot Era)** | 2025‑中           | ✅ 是（有限披露） | 官方称使用 DexVerse 平台生成混合物理仿真与 synthetic 操作流水进行训练                                        | 仿真为主           | 虽未详述轨迹数据细节，但明确使用 synthetic training ([Hugging Face](https://huggingface.co/papers?q=Vision-Language-Action+%28VLA%29+model&utm_source=chatgpt.com "Daily Papers"))                                                                                                                                                                                                                                      |
| **VQ‑VLA**             | 2025‑07          | ✅ 是       | 训练 vector‑quantized action tokenizer 时利用大量**合成动作轨迹**，基于合成数据规模达前代 >100×，domain gap 较小 | 隶属 OpenVLA 微调步骤  | 隐含使用大规模合成轨迹训练 tokenizer，提升多任务泛化与推理速度 ([arXiv](https://arxiv.org/abs/2507.01016?utm_source=chatgpt.com "VQ-VLA: Improving Vision-Language-Action Models via ..."), [Moonlight](https://www.themoonlight.io/en/review/vq-vla-improving-vision-language-action-models-via-scaling-vector-quantized-action-tokenizers?utm_source=chatgpt.com "[Literature Review] VQ-VLA: Improving Vision-Language- ...")) |
| **GO-1（智元启元大模型）** | 2025-03          | ✅ 是       | 使用仿真平台（如NVIDIA Isaac Sim）生成大规模合成操作数据，结合少量遥操作微调                                       | 少量真实遥操作用于微调    | 官方未披露合成数据比例，但确认“合成为主，真实为辅”                                                                                                                                                                                                                                                                                                                                                                              |
| **OneTwoVLA**          | 2025-05          | ✅ 是       | 三步合成流水线：<br>1. **文本场景生成**：用 Gemini 2.5 Pro 生成 1.6 万条家庭物品布局描述；<br>2. **图像渲染**：FLUX.1-dev 生成带鱼眼畸变、机械臂视角的合成图像；<br>3. **任务与推理生成**：Gemini 2.5 Pro 生成长周期任务指令与结构化推理内容（含计划、历史、恢复策略）。覆盖 180 种 household 物品，包含机器人未见过的新物体。 | 200-600 组真实机器人示范用于微调 | 清华大学与上海 AI Lab 联合提出，首次实现“系统一”（快速动作）与“系统二”（慢速推理）的**统一模型架构**，通过自适应推理机制在关键节点触发高层推理，其余时间直接执行动作，解决延迟与认知断层问题。支持长周期任务规划（如番茄炒蛋、火锅配菜）、动态错误恢复、自然人机交互和开放世界视觉 grounding。在仿真中显著优于 π0 与传统双系统设计。项目与论文已开源。<br>**论文**：[arXiv:2505.11917](https://arxiv.org/abs/2505.11917)<br>**项目页**：[https://one-two-vla.github.io/](https://one-two-vla.github.io/) |
## 🌟 合成数据训练方法分类与技术归纳

### ✅ 1. 仿真轨迹生成（Simulated Trajectories）

- **核心**：借助 MuJoCo、Isaac Sim 等物理仿真平台，通过域随机化生成大规模动作轨迹。
    
- **典型应用**：GraspVLA 的 SynGrasp‑1B 数据集，支持多光照、多物体、多背景训练，广泛适用于零样本抓取场景 ([turn0search3](https://arxiv.org/html/2503.03464v1))。
    
- **优势**：高样本规模、丰富多样、成本低。
    
- **挑战**：现实感欠缺，物理交互细节如摩擦和柔顺性难以模拟，需 sim‑to‑real 调整 ([turn0search11](https://pmc.ncbi.nlm.nih.gov/articles/PMC9038844/))。
    

---

### ✅ 2. 世界模型＋神经轨迹生成（World Model + Neural Trajectories）

- **核心**：使用 Cosmos Predict‑2 / Reason 等世界模型生成“梦境视频”，再通过逆动力学模型生成 3D 动作轨迹。
    
- **典型应用**：GR00T‑Dreams 管道用于训练 GR00T N1.5，36 小时生成上百小时轨迹，显著提升泛化能力 ([turn0search1](https://developer.nvidia.com/blog/enhance-robot-learning-with-synthetic-trajectory-data-generated-by-world-foundation-models/))。
    
- **优势**：可以在少量示范下跨任务扩展生成新行为；语言提示生成灵活可控。
    
- **挑战**：缺乏真实物理约束，难以处理复杂接触或变形交互。
    

---

### ✅ 3. 真实‑仿真‑真实闭环合成视频（ReBot Pipeline）

- **核心**：将真实轨迹 replay 到仿真环境中，并与真实背景融合生成合成训练样本。
    
- **典型应用**：OpenVLA 和 Octo 用此管道增强数据后，真实场景下成功率分别提升 ~20% 和 ~17% ([turn0search2](https://arxiv.org/html/2507.01925v1))。
    
- **优势**：结合真实动作与环境多样性，有效缩小 sim‑real gap。
    
- **挑战**：需要真实轨迹配合仿真执行流程，生成管道较复杂。
    

---

### ✅ 4. 仿真强化学习（Simulation Reinforcement Learning）

- **核心**：在仿真环境中通过 RL 训练控制策略，生成长期复杂任务行为。
    
- **典型应用**：Psi‑R1 在仿真中执行麻将类长时程任务，使用 synthetic data 驱动行为学习。
    
- **优势**：适合长期、复杂任务；可生成丰富策略数据。
    
- **挑战**：政策的 sim‑to‑real 转移难度大，需要额外迁移机制。
    

---

### ✅ 5. 域随机化与高质量渲染（Domain Randomization & Rendering）

- **核心**：利用光照、纹理、视角、背景等参数随机化生成多样训练样本。
    
- **典型应用**：SynGrasp‑1B、Helix 等均采用随机化训练场景。
    
- **优势**：提高模型对环境变化的鲁棒性。
    
- **挑战**：设计不当会产生非自然样式；仍需实验证明其泛化上限。
    

---

### ✅ 6. 动作量化标记与 Tokenizer（Action Tokenization）

- **核心**：使用大量合成轨迹训练量化动作标记器（如 VQ‑VLA），将动作转换为离散 token 供 VLA 生成模型使用。
    
- **典型应用**：VQ‑VLA 在 VLA 模型中实现更快推理与长时动作生成 ([turn0search0](https://arxiv.org/html/2507.01925v1), [turn0search3](https://arxiv.org/html/2503.03464v1)).
    
- **优势**：提升动作推断速度与连续性，对长任务控制尤为有效。
    
- **挑战**：需要海量训练数据；token 设计复杂。
    

---

### ✅ 7. 潜动作表示预训练（Latent Actions from Video）

- **核心**：使用 VAE/VQ-VAE/bin编码器自动从常规互联网视频中提取潜动作特征，并进行迁移训练。
    
- **典型应用**：LAPA 框架（隐性动作预训练）适用于没有机器人标签的预训练方式。
    
- **优势**：数据来源广泛，无需昂贵的标签或仿真环境。
    
- **挑战**：视频与动作对齐困难，迁移到机器人动作存在缺失。
    

---

### ✅ 8. 多模态世界模型视频合成（World Model Video Synthesis）

- **核心**：利用 Diffusion + ControlNet 等模型生成视觉控制条件（边缘图、深度图、语义标签等），将结构化数据转化为高保真视频。
    
- **典型应用**：Cosmos Transfer 结合 GR00T 等使用，实现结构控制向动作映射 ([turn0search1](https://developer.nvidia.com/blog/enhance-robot-learning-with-synthetic-trajectory-data-generated-by-world-foundation-models/)).
    
- **优势**：保证视觉与语义一致性，适用于多模态训练。
    
- **挑战**：渲染 fidelity 高昂，对生成策略与逻辑验证要求高。
    

---

## 🧠 综合对比总结

可以看到，VLA 合成数据训练方法主要分为以下几大类，每类技术在 VLA 模型中都有具体落地案例：

- **从仿真直接生成轨迹**（如 GraspVLA）
    
- **由语言驱动生成神经轨迹**（如 GR00T‑Dreams）
    
- **真实轨迹生成合成样本**（如 ReBot）
    
- **借助强化学习的策略仿真训练**（如 Psi‑R1）
    
- **域随机化与渲染增强多样性**（如 Helix、SynGrasp‑1B）
    
- **动作 tokenization**（如 VQ‑VLA）
    
- **从视频中提取潜在动作表示**（如 LAPA）
    
- **多模态世界模型驱动的视频合成**（如 Cosmos Transfer）
    

这些方法各有技术重点，但共通目标是通过合成数据提升训练规模、多样性与泛化能力，克服真实演示难以充分覆盖的瓶颈。如你希望对某个方法进一步深入分析其 pipeline、开源代码或训练效果评估，可以告诉我具体方向，我可进一步细化展开！