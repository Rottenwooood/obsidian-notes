## 路径依赖
- 找论文
	- 公众号
	- https://github.com/TianxingChen/Embodied-AI-Guide
	- https://paj5uamwttr.feishu.cn/wiki/GaozwfU3iiWA9Nk0LVUcUzKZnHc
- 规划
	- 信息汇总
	- 安排任务
	- 排序
## 问AI
- 现有vla似乎是用了预训练的视觉模型来训练
	- 能不能通过去相机畸变/去噪声/处理画面/用专门的视角来特化画面来改善性能
	- 能不能通过👆从零训练特定模型或者微调模型来改善性能
	- 能不能用yolo预先识别，分割，加强然后作为输入，改善性能
	- 其他影响因素
		- pid参数
- **真实环境反馈延迟**（机械臂执行慢）
	- 加入**动作平滑模块**（如低通滤波
	- 测试延迟差距
	- 手动给仿真加上延迟，微调模型
		- 看能不能感觉到时序信息
- 看能不能比较时间上先后
## 论文待看
- [ ] 
- [ ] https://arxiv.org/abs/2203.12601?spm=a2ty_o01.29997173.0.0.4158c921ZCcyvD&file=2203.12601 #论文 #AI 
- [ ] GAN
- [ ] RT-2
- [ ] 了解BC和IL
## 杂
- [ ] 针对Lerobot的硬件特性（如摄像头型号），设计轻量级适配器（1个全连接层）
- [ ] diffusion
- 推理的时候也是只拍一次图片，然后输出动作序列吗还是每隔多少毫秒拍一次照片，然后生成一次动作序列，再修正或者只生成一小段动作
-  大模型我理解，多模态我也理解，但是VLA是怎么工作的，我说的是输入输出。我记得比如openvla名义上是通过图像和指令输入VIT和tokenizer，混合后再输入微调后的VLM，再输出为动作（特殊的token）（n维向量）。几个问题是：它是根据什么更新权重的？经典的深度学习和大模型根据监督学习，标记的数据来更新，强化学习根据真实值与预期值差距更新Q网络，VLA怎么更新VIT和VLM。
- PyBullet
- moveit
- 调查有哪些会议
- 基于语义理解的域随机化
	- 指令：把左边的方块拿起来
		- “左边”指桌子左半边
	- 指令：把桌子上的方块拿起来
		- “桌子上”指整个桌子
- lerobot软件库
- 月食 9.8
- 耳机 找有线的