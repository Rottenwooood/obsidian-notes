## 参考
https://datawhalechina.github.io/joyrl-book/
## 笔记
### 001强化学习方向概述
#### 多智能体强化学习
- 新问题：
	- 非静态问题：环境的状态不仅取决于当前智能体的动作，同时取决于其他智能体的动作
	- 信号问题：智能体之间的合作和竞争等：如何高效的通信并从信号中学习
	- 信誉分配问题：在合作任务中确定各个智能体的责任或贡献
#### 从数据中学习
- 包括：
	- 模仿学习IL：从专家数据中来学习
		- 问题：奖励函数难以定义或策略本身不好学出来
		- 解决：模仿人类的行为来得到一个好的policy
		- 例如行为克隆BC，将每一个状态-动作对视为一个训练样本，并使用监督学习的方法（如神经网络）来学习一个策略。但这种方法容易受到分布漂移的影响
	- 逆强化学习IRL：从人类数据中学习奖励函数
		- 观察人类的行为得到一个奖励函数，利用这个奖励函数去做强化学习
		- 缺点：会受到噪声数据的影响
	- 从人类反馈中学习RLHF：从人类标注的数据中微调
	- 离线强化学习
	- 世界模型
#### 探索策略
- 在探索和利用之间作出权衡，避免局部最优解，提高智能体的鲁棒性
	- 常用算法：xigema-greedy，置信上界UCB
	- 结合进化算法以提高效率：NEAT，PBT
#### 实时环境
- 问题：实时环境中训练的效率非常低，且带来安全隐患
- 解决：离线强化学习
	- 离线环境中训练模型，部署在在线环境中
- 新问题：两个环境之间可能存在分布漂移，导致训练好的模型在在线环境中出现意外
- 解决：世界模型
	- 将环境分为世界模型和控制器，世界模型预测下一个状态，控制器根据当前状态作出动作
- 新新问题：世界模型的预测错误导致控制器决策错误，需要**提高预测精度**
#### 多任务强化学习
- 有时同时需要解决多个问题，如何在之中做权衡
	- 联合强化学习
		- 多个任务的奖励加权求和
	- 分层强化学习
		- 多个任务的决策分为高层和低层
#### 强化学习与深度学习的关系
- 大脑与眼睛
- 决策与感知
### 002 马尔科夫决策过程
- MDP
- 序列决策问题的本质是在与环境交互的过程中学到一个目标的过程
- ![[Pasted image 20250817211339.png]]
- 马尔科夫性质
	- 在给定历史状态的情况下，未来状态只与当前状态有关，与过去状态无关
	- 实际问题中很多问题不满足马尔科夫性质
		- 结合其他的方法辅助决策
- 回报Return
	- ![[Pasted image 20250817211703.png]]
	- 折扣因子伽马0<<1
	- ![[Pasted image 20250817211756.png]]
- 有限状态与无限状态
	- 有限
	- 无限
		- 用柏松过程进行建模（联想：柏松分布？）
- 状态转移矩阵
	- 状态转移概率：从S1到Sn转移的概率
	- ![[Pasted image 20250817212014.png]]
	- 状态转移矩阵是环境的一部分，跟智能体是没什么关系的，而智能体会根据状态转移矩阵来做出决策
	- 马尔科夫过程+奖励=马尔科夫奖励过程，马尔科夫奖励过程+动作=马尔科夫决策过程
	- 把马尔可夫决策过程描述成一个今天常用的写法，即用一个五元组 <S,A,R,P,γ> 来表示。其中 S 表示状态空间，即所有状态的集合，A 表示动作空间，R 表示奖励函数，P 表示状态转移矩阵，γ 表示折扣因子
### 003 动态规划
- 动态规划
	- 最优化原理
		- 问题的最优解包含的子问题的解也是最优的
	- 无后效性
		- 马尔科夫性质
	- 有重叠子问题
- 状态价值函数
- 动作价值函数
- 贝尔曼方程
	- ![[Pasted image 20250818142126.png]]
	- ![[Pasted image 20250818142138.png]]
- 策略迭代和价值迭代
	- 策略迭代
		- ![[Pasted image 20250818141217.png]]
	- 价值迭代
		- ![[Pasted image 20250818141240.png]]
	- 区别
	- ![[Pasted image 20250818141034.png]]
	- 策略迭代在迭代V和pi之间来回跳变，而价值迭代只迭代V，理论上策略迭代更快
### 004 免模型预测
- 有模型和免模型
	- model based & model free
	- 
	- 有模型：动态规划，世界模型，PlaNet，Dreamer等
		- 尝试先学习一个环境的模型，再利用这个模型计划计算最佳的行动策略
		- 优点：在不与真实环境交互的情况下学习，节省实验成本
		- 缺点：模型往往不够完美，或是复杂到难以计算
	- 免模型：其他方法
		- 直接从环境的交互中学习
- 预测与控制
	- 预测：估计或计算环境中的某种策略的期望值
	- 控制：找到一个最优策略，可以最大化期望的回报
- 蒙特卡洛
	- 只适用于有终止状态的马尔科夫过程
	- 采样大量轨迹，对每个轨迹计算回报，取平均近似，即经验平均回报
	- 分为FVMC和EVMC
	- ![[Pasted image 20250818145547.png]]
	- 实际的更新
		- ![[Pasted image 20250818145608.png]]
	- FVMC是一种基于回合的增量式方法，具有无偏性和收敛快的优点，但是在状态空间较大的情况下，依然需要训练很多个回合才能达到稳定的结果。而EVMC则是更为精确的预测方法，但是计算的成本相对也更高。
- 时序差分
	- 基于经验的动态规划方法
	- 单步时序差分
		- 将问题分解为只涉及一步的预测，简化运算
	- 差异
		- 时序差分方法可以在线学习（- ），每走一步就可以更新，效率高。蒙特卡洛方法必须等游戏结束时才可以学习。
	- 时序差分方法可以从不完整序列上进行学习。蒙特卡洛方法只能从完整的序列上进行学习。
	- 时序差分方法可以在连续的环境下（没有终止）进行学习。蒙特卡洛方法只能在有终止的情况下学习。
	- 时序差分方法利用了马尔可夫性质，在马尔可夫环境下有更高的学习效率。蒙特卡洛方法没有假设环境具有马尔可夫性质，利用采样的价值来估计某个状态的价值，在不是马尔可夫的环境下更加有效。
- n步时序差分
### 005 免模型控制
- 探索策略
	- 自举依赖于先前的估计值，因此这可能会导致估计出的价值函数存在某种程度上的偏差
	- xigema-greedy策略
		- 1-xigema的概率执行预定动作，xigema的概率执行随机动作
		- 在实践中，xigema会逐渐衰减
		- 
- Q-learning
	- ![[Pasted image 20250818171631.png]]
	- ![[Pasted image 20250818171656.png]]
- Sarsa
	- ![[Pasted image 20250818171710.png]]
	- ![[Pasted image 20250818171721.png]]
- 二者区别：Q-learning每次更新用的是理论上最优的动作，但是没有考虑探索导致的风险。Saras则每次更新用的是实际上选择的动作，既可能是理论最优动作，也可能是随机选择的动作
- 同策略和异策略
	- ![[Pasted image 20250818171813.png]]
### 007 DQN
- 用深度网络来拟合Q网格
	- 优点：能够处理非离散值
	- ![[Pasted image 20250819142140.png]]
- 经验回放
	- 问题：深度学习中假设了各个样本之间是独立同分布的，但是强化学习中不满足此条件
	- 解决：设置一个样本池，每次取样存到这里，随机抽取部分样本进行更新
- 目标网络
	- 额外的目标网络，每隔好几步才更新一次，增加训练的稳定性
### 008 DQN进阶
- Double DQN
	- 把动作选择和动作评估分离开来，
	- ![[Pasted image 20250819163726.png]]
	- 并且是使用两个网络，每次随机选择一个网络来更新
- Dueling DQN
	- 在输出层前分流出两个层，价值层和优势层
	- ![[Pasted image 20250819163856.png]]
	- ![[Pasted image 20250819163908.png]]
	- 使得目标值更容易计算，因为通过使用两个单独的网络，我们可以隔离每个网络输出上的影响，并且只更新适当的子网络，这有助于降低方差并提高学习鲁棒性。
- Noisy DQN
	- 在神经网络中引入噪声层，增强探索能力
- PER DQN
	- 优先经验回放
		- 根据TD误差（当前网络Q值与目标网络Q值的差值）的大小选取样本，
		- 实现：SumTree
	- 随机优先级采样
		- 不再是直接采样TD 误差最大的样本，而是定义一个采样概率
		- ![[Pasted image 20250819170432.png]]
		- 即使对于最低优先级的样本，我们也不希望它们的采样概率为0 ，因此我们可以在优先级上加上一个常数
		- ![[Pasted image 20250819170503.png]]
		- 其他的优先级计算方式
		- ![[Pasted image 20250819170536.png]]
	- 重要性采样
		- 通过与待估计分布不同的另一个分布中采样，然后通过采样样本的权重来估计待估计分布的性质
		- ![[Pasted image 20250819170729.png]]
		- 
- C51
- RainBow DQN
### 009 策略梯度算法
- 即policy-based,之前的DQN是value-based
- 直接**对策略本身进行近似优化**。在这种情况下，我们可以将策略描述成一个带有参数θ的连续函数，该函数**将某个状态作为输入，输出的不再是某个确定性（ deterministic）的离散动作，而是对应的动作概率分布**，通常用πθ(a|s)表示，称作随机性（ stochastic）策略
- value-based的缺点
	- 无法表示连续动作
	- 高方差
	- 探索与利用的平衡问题
- 策略的价值期望公式
	- ![[Pasted image 20250820162640.png]]
- 我们的目标是最大化策略的价值期望，因此要求关于参数θ的梯度，用于更新策略
- ![[Pasted image 20250820162829.png]]
- 基于价值的算法是通过学习价值函数来指导策略的，而基于策略的算法则是对策略进行优化，并且通过计算轨迹的价值期望来指导策略的更新。
- 前者是概率随机选择动作，概率选择最优动作，后者是每个动作都有概率选择，但是随着更新，更好的动作选择概率更大

- REINFORCE算法
	- 即蒙特卡洛策略梯度算法
	- 采样N条轨迹，计算其梯度的平均值来近似策略的梯度
- 策略梯度推导进阶
	- 平稳分布
		- 无论初始状态是什么，经过多次概率转移之后都会存在一个稳定的状态分布。其次我们只需要知道这个稳定的分布并乘以对应的价值，就可以计算所谓的长期收益了。
		- ![[Pasted image 20250820170913.png]]
		- 对任意马尔科夫链，满足非周期性和状态连通性时，即存在平稳分布
	- ![[Pasted image 20250820171103.png]]
- 策略函数的设计
	- 离散动作
		- 更改网络模型，在模型输出层加一层softmax，使其归一化为0-1的数值
	- 连续动作
		- ![[Pasted image 20250820172315.png]]
### 010 Actor-Critic
- Actor-Critic相当于是策略梯度算法，但是不使用Gt来更新策略，额外使用了一个价值网络来评估局部的动作的优势，优势=状态动作对价值-状态价值（平均状态动作对价值），有利于减小误差，仍使用策略梯度来更新，并且能及时更新策略网络,从而缓解高方差问题（A2C）
- A3C:
	- 原先的A2C算法相当于只有**一个全局网络**并持续与环境交互更新。而 A3C 算法中**增加了多个进程，每一个进程都拥有一个独立的网络和环境以供交互，并且每个进程每隔一段时间都会将自己的参数同步到全局网络中**，这样就能提高训练效率。
	- 这种训练模式也是比较常见的**多进程训练**模式，也能用于其他算法中，也包括前面讲到的基于价值的算法
- 一种改进
	- 广义优势估计（GAE）
		- 使用时序差分和蒙特卡洛的折中方法来更新
- 实践上，由于Actor和Critic网络的输入一样，常将二者和二为一
	- ![[Pasted image 20250820213520.png]]
	- ![[Pasted image 20250820213534.png]]
### 011 DDPG
- 相当于连续动作空间版本的DQN算法，并且只适用于连续动作空间，使用确定性策略
- 深度确定性策略梯度算法（deep deterministic policy gradient，DDPG）
- DPG
	- 使用Actor代替xigema-greedy策略函数，但是Actor输出的是确定性的动作值（最优的动作），而非选择各个动作的概率，也不做梯度更新
- DDPG
	- 引入目标网络，经验回放
	- 在输出动作上引入OU噪声
		- 在复杂环境中优于高斯噪声，稳定，自回归，平滑
		- ![[Pasted image 20250820221930.png]]
		- 其中 xt 是 OU 过程在时间 t 的值，即当前的噪声值，这个 t 也是强化学习中的时步（ time step ）。μ 是回归到的均值，表示噪声在长时间尺度上的平均值。θ 是 OU 过程的回归速率，表噪声向均值回归的速率。σ 是 OU 过程的扰动项，表示随机高斯噪声的标准差。dWt 是布朗运动（ Brownian motion ）或者维纳过程（Wiener process ），是一个随机项，表示随机高斯噪声的微小变化。
		- 需调的超参数： μ 和σ 
	- 优点：
		- 缓解在连续动作空间中的高方差问题
		- 梯度更新相对高效，可处理高维的状态空间和动作空间
		- 经验回放和目标网络
	- 缺点
		- 只适用于连续动作空间
		- 高度依赖超参数
		- 高度敏感的初始条件
		- 容易陷入局部最优
- TD3
	- TD3 算法，英文全称为 twin delayed DDPG，中文全称为双延迟确定性策略梯度算法
	- 改进：
		- 双Q网络
			- 在DDQN的Critic网络上再加一层，形成两个Critic网络，计算TD误差时取较小的一个
			- 和Double DQN一样，可以减少Q值的过估计，提高算法稳定性和收敛性
		- 延迟更新
			- 让Actor的更新频率比Critic低一个数量级
		- 噪声正则
			- 也即目标策略平滑化
			- 延迟更新治标不治本，只是减少Critic影响到Actor的频率，而非减少Critic出现偏差的概率
			- 给Critic网络加上一个噪声，提高其抗干扰能力
### 012 PPO
- PPO（Proximal Policy Optimization，近端策略优化）
	- on policy算法
- 利用了重要性采样来优化策略梯度更新，提高Actor-Critic架构的稳定性
- ![[Pasted image 20250821095817.png]]
- 一句话：PPO相对于A2C，收集一批数据之后，进行多个Epoch的策略的更新，但是加上了Clip确保策略每次变化的幅度不是很大，并且面对旧数据，使用重要性采样来修正，使其能够用来评估较新策略的期望
### 013 SAC
- 相比于A2C，将目标改为了最大化期望奖励和策略熵，后者所占的比例由温度系数调节（训练过程中自动调节）
	- A2C 可能在局部最优来回试探，但不会“主动冒险”去探索新区域。
	- SAC 会主动尝试未访问的状态，因为“不确定性高 → 熵大 → 收益高”。
- A2C实现探索靠的是策略输出的随机性（有噪声的动作），而非主动鼓励探索

### 稀疏奖励
多数情况下智能体不能及时得到奖励，这种情况下训练很困难
- 设计奖励
	- 根据领域知识，去设计合理的奖励
- 好奇心
	- 设置内在好奇心模块提供额外的奖励
		- 未来状态越难以预测，奖励越大
	- 特征提取器
		- 与要采取动作无关的特征会被屏蔽掉
- 课程学习
	- 给任务分阶段设置几个简单的任务，逐步学习
	- 逆向课程生成
		- 根据目标状态，设计若干个与其相近的状态，选取能够得到始终奖励的情况，取若干略不相近的状态
- 分层强化学习
	- 每一个智能体都把上层的智能体所提出的愿景当作输入，决定他自己要产生什么输出。
	- 将一个复杂的强化学习问题**分解成多个小的、简单的子问题**，每个子问题都可以单独用马尔可夫决策过程来建模。这样，我们可以将智能体的策略分为**高层次策略和低层次策略**，高层次策略根据当前状态决定如何执行低层次策略。这样，智能体就可以解决一些非常复杂的任务
	- ![[Pasted image 20250824120347.png]]
	- ![[Pasted image 20250824120400.png]]
	- 疑问：
		- 高层次的智能体如何训练