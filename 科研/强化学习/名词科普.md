## 科普
[机器人强化学习深度科普 - 知乎](https://zhuanlan.zhihu.com/p/20664700952)
## 笔记
- 关于强化学习的分类有很多种方法，大致我们可以将其分类成以下几类：**基于价值函数的RL**和**基于策略的RL**；**Online RL**和**Offline RL**；**On-Policy**和**Off-Policy**；**Model-based** RL和**Model-free** RL。
---

### **一、强化学习（RL）的核心分类体系**  
#### **1. 基于优化目标的分类**  
| **类型**             | **核心思想**                                                                 | **典型算法**                          | **适用场景**                              | **机器人领域应用**                              |
|----------------------|-----------------------------------------------------------------------------|---------------------------------------|------------------------------------------|-----------------------------------------------|
| **Value-based RL**   | 学习状态/动作的价值函数（Q函数），通过价值评估选择动作                          | Q-Learning, SARSA, DQN                | **离散动作空间**（如棋类、游戏）           | 较少（机器人动作多为连续空间）                   |
| **Policy-based RL**  | 直接优化策略参数（神经网络），最大化累积奖励                                    | REINFORCE, PPO, TRPO                  | **连续动作空间**（如机器人控制）           | **主流**（PPO成机器人RL“标配”）                 |
| **Actor-Critic**     | 结合两者：Critic评估价值，Actor优化策略                                        | A3C, A2C, DDPG, SAC                   | 高维状态/连续动作空间                     | **广泛使用**（平衡样本效率与稳定性）             |

> **关键区别**：  
> - Value-based：目标函数为最小化 **Bellman Error/TD Error**（误差驱动）  
> - Policy-based：目标函数为最大化 **累积奖励**（奖励驱动）  
> - *争议点*：并非严格按动作空间划分（SAC可处理连续空间，Policy Gradient也可用于离散空间），但**连续控制任务中Policy-based更主流**。

---

#### **2. 基于数据使用方式的分类**  
| **类型**         | **数据交互方式**                        | **数据效率** | **核心机制**          | **机器人适用场景**                  |
| -------------- | --------------------------------- | -------- | ----------------- | ---------------------------- |
| **Online RL**  | 实时与环境交互生成新数据                      | 低        | 需持续采集数据           | 游戏/仿真环境（数据无限）                |
| **Offline RL** | 仅用**预先收集的静态数据集**训练，**不交互**        | **极高**   | 避免环境交互成本          | **工厂车间等数据稀缺场景**（如工业机器人）      |
| **On-policy**  | 仅用**当前策略生成**的数据更新策略               | 低        | 无数据缓存池（“自来水洗手”）   | 仿真环境数据充足时                    |
| **Off-policy** | 可用**历史/其他策略**数据更新（如Replay Buffer） | **高**    | 数据缓存池循环利用（“脸盆接水”） | **机器人主流**（提升样本效率，如DDPG, SAC） |

> **决策逻辑**：  
> - 数据**极其昂贵**（如真实机器人实验）→ **Offline RL**  
> - 数据**较宝贵但可重复利用** → **Off-policy Online RL**  
> - 数据**无限且易获取**（如游戏）→ **On-policy Online RL**  

---

#### **3. 基于环境模型的分类**  
| **类型**             | **是否显式建模环境**                     | **优势**                          | **劣势**                              | **机器人领域现状**                              |
|----------------------|--------------------------------------|---------------------------------|-------------------------------------|---------------------------------------------|
| **Model-based RL**   | **是**（学习状态转移/奖励函数）             | 理论上**样本效率高**，可做规划推演         | 模型误差累积，优化复杂                     | **效果有限**（80%+成功案例用Model-free）       |
| **Model-free RL**    | **否**（直接从交互数据学习策略）            | 避免建模误差，**实现简单**               | 需大量交互数据                           | **主流**（尤其Sim2Real路线）                   |

> **关键澄清**：  
> - **Sim2Real RL ≠ Model-based RL**：  
>   - *张伟楠观点*：Sim2Real依赖仿真器（视为“模型”），属Model-based。  
>   - *宋运龙观点*：若仿真器仅用于**数据生成**而非策略优化（如PPO训练），仍属**Model-free**。  
> - **Model-based的理想 vs 现实**：  
>   - *理想*：减少Sim2Real差距、降低样本需求、模型复用。  
>   - *现实*：手工仿真器与真实环境差异大，模型误差导致性能不稳定。  

---

### **二、机器人RL的核心挑战与技术路线**  
#### **1. Sim2Real（仿真到现实）问题**  
- **根源**：RL需海量交互数据 → 真实机器人试错成本过高（易损坏设备）。  
- **Reality Gap（现实差距）**：仿真器无法精确模拟接触/碰撞等物理细节。  
- **两大解决方案**：

| **方法**                | **原理**                                                                 | **机器人应用案例**                     |
|-------------------------|------------------------------------------------------------------------|--------------------------------------| 
| **领域随机化（Domain Randomization）** | 在仿真中**随机化环境参数**（地形/摩擦力等），使策略鲁棒化                              | 四足机器人（如波士顿动力Spot）、工业机械臂       |
| **领域自适应（Domain Adaptation）**  | 95%训练在仿真 + **5%真实数据微调**；或用真实数据校准仿真器参数                          | 高精度操作任务（如灵巧手抓取）                |
#### **2. 师生网络学习（Teacher-Student Learning）**  
- **问题**：真实机器人只能获取局部观测（如腿部传感器），但仿真中可获取全局信息（“作弊信息”）。  
- **解决方案**：  
  1. **Teacher**：在仿真中利用**全局信息**（特权信息）训练高性能策略。  
  2. **Student**：通过**知识蒸馏**，将Teacher策略迁移到仅依赖局部观测的Student策略。  
- **典型应用**：四足机器人盲走复杂地形（Science Robotics 2020论文）。  

#### **3. 任务类型与RL必要性**  
| **任务类型**    | **是否需要RL** | **原因**                             | **案例**            |
| ----------- | ---------- | ---------------------------------- | ----------------- |
| **静态执行**    | **否**      | 动作与环境交互弱（如重力主导），路径规划+执行即可完成        | 简单夹取、叠衣服（非动态甩动）   |
| **动态控制**    | **是**      | 需高频实时调整动作（>100Hz），传统规划无法处理动态交互     | 机器狗奔跑、无人机竞速、灵巧手操作 |
| **长时序任务规划** | **是**      | 任务拆解复杂（如“做饭”），奖励函数难以显式设计，需RL探索策略空间 | 厨房机器人、系鞋带机器人      |

> **关键结论**：  
> - **Locomotion（运动）**：RL核心优势（动态控制，如机器狗踩石头自适应调整）。  
> - **Manipulation（操作）**：**感知是瓶颈**（需理解物体物理属性），RL仅解决控制层问题。  
> - **无人机竞速**：RL优于MPC（当任务无法分解为显式轨迹时，如极限竞速）。  

---

### **三、领域专家核心观点提炼**  
#### **张伟楠教授（上海交大）**  
1. **RL分类本质**：  
   - Value-based vs Policy-based：**优化目标不同**（误差 vs 奖励）。  
   - Online/Offline：**是否实时交互**；On/Off-policy：**数据来源是否与当前策略一致**。  
2. **机器人RL现状**：  
   - Sim2Real + PPO 是**运动任务主流**（如四足机器人）。  
   - **师生网络绕过状态估计难题**：Teacher用全局信息训练，Student仅需局部观测。  
3. **历史视角**：  
   - Q-Learning早期被质疑（“无模型=不可行”），深度学习崛起后成为DRL核心（DQN突破高维输入）。  

#### **宋运龙博士（苏黎世大学）**  
1. **算法选择逻辑**：  
   - **机器人首选PPO**（On-policy）：连续控制任务中稳定高效，开源工具成熟（Stable Baselines）。  
   - SAC等Off-policy算法**理论样本效率高，但实际训练慢**（GPU数据吞吐瓶颈）。  
2. **无人机 vs Manipulation**：  

| **维度**         | **无人机/Locomotion**                          | **Manipulation**                          |
|------------------|---------------------------------------------|-----------------------------------------|
| **感知重点**     | 自身状态（位姿/速度）                           | **环境物体属性**（形状/重量/材质）               |
| **核心挑战**     | 动态控制精度                                  | **通用物体理解**（接近AGI难题）                 |
| **RL必要性**     | 高（动态避障需实时决策）                        | 低（控制层简单，感知层是瓶颈）                   |
3. **可微分仿真（Differentiable Simulation）**：  
   - 通过可微引擎直接计算梯度，**训练速度提升10-100倍**。  
   - **局限**：Locomotion数据成本低（优化意义小），但**视觉输入任务潜力大**（端到端RL瓶颈）。  

#### **补充观点（2024年末博客）**  
- **RL与Control本质统一**：  
  > “RL和Control解决相同问题——对动态系统设计策略。RL的MDP框架（S,A,R,S’）与Control的数学表述无本质差异，只是解法不同。”  
- **Sim2Real RL定位争议**：  
  - 若仿真器用于**策略训练** → 属Model-based；  
  - 若仅用于**生成离线数据集** → 属Model-free（更符合当前实践）。  

---

### **四、关键结论与趋势**  
1. **算法选择指南**：  
   - **数据稀缺**（真实机器人）→ Offline RL / 领域自适应  
   - **连续控制**（运动/操作）→ PPO（On-policy）为主流  
   - **仿真训练** → 领域随机化 + 师生网络（解决观测差距）  
2. **领域差异**：  
   - **Locomotion**：Model-free Sim2Real已成熟（四足机器人商业化）。  
   - **Manipulation**：**感知突破是前提**（大模型提供环境理解，RL仅解决控制层）。  
   - **无人机**：竞速等动态任务RL优于MPC，但常规导航仍用MPC。  
3. **未来方向**：  
   - **可微分仿真**：解决视觉输入RL的效率瓶颈。  
   - **大模型+RL**：用LLM/VLM提供高层任务规划与物体理解（如“用大模型生成Manipulation的子目标”）。  
   - **Offline RL**：降低真实机器人训练成本的核心路径。  

> **专家共识**：  
> - “RL内部差异（如Sim2Real vs Offline RL）远大于RL与Control的差异。”  
> - “**感知决定Manipulation上限，控制决定下限**——没有感知的RL在操作任务中寸步难行。”  

---
**参考文献**  
- 张伟楠：[wnzhang.net](http://wnzhang.net) | 《动手学强化学习》  
- 宋运龙：[yun-long.github.io](https://yun-long.github.io) | 无人机竞速论文：*Reaching the Limit in Autonomous Racing*  
- 经典论文：*Learning quadrupedal locomotion over challenging terrain* (Science Robotics 2020)  
- 工具：Flightmare仿真器、Differentiable Simulation for Locomotion  

> 注：本文严格区分技术概念（如On/Off-policy与Online/Offline），避免常见误解（如“Value-based=离散控制”），并突出机器人场景的特殊约束（数据成本、感知瓶颈）。