## 参考
https://zhuanlan.zhihu.com/p/1911573411353822529
https://grok.com/chat/06cd25a6-a084-4d0f-9d4c-8245ea1bb958
## 笔记
- 三大挑战：
	- 样本复杂度高：使用RLPD算法
	- 优化稳定性差：使用预训练的视觉骨干模型
	- 奖励函数设计难
- 模仿学习依赖人类演示，无法从错误中学习或超越人类水平，之前的RL主要依赖演示，无法处理从零开始的困难任务
- 引入人类在环Human-in-the-loop，RL能够在1-2.5小时内训练出近完美成功率（接近100%）和超人类速度（平均快1.8倍）的策略。
- 适用于多样任务，包括动态操纵（如翻转锅中物体）、精密组装（如主板安装）和双臂协调（如计时带组装）
- 概述：
	- 分为三个主要组件：
		- **Actor进程**：与环境交互，执行当前策略，收集数据。如果需要，人类可以通过SpaceMouse（3D鼠标）干预机器人控制。环境模块化，支持多相机、多臂机器人和各种控制器。
		- **Learner进程**：从缓冲区采样数据，使用RLPD算法（基于Ball et al., 2023）更新策略，并定期发送给Actor。RLPD是一种离策略RL，能均匀采样先验数据（演示）和在线数据。
		- **回放缓冲区**：分为**演示缓冲区**（存储20-30个人类演示）和**RL缓冲区**（存储在线数据，包括策略过渡和人类校正）。
	- 观察空间𝐬包括图像、机器人本体感知（如末端执行器姿态）；动作空间𝐚包括连续动作（如末端扭矩）和离散动作（如夹持器开/关）。
	- 训练流程（如图3）：先训练奖励分类器，然后收集演示，最后在线RL训练中引入人类校正。
- 预训练视觉骨干
	- 使用预训练的ResNet-10模型（在ImageNet上训练）处理多相机图像（分辨率128x128）。
	- 优势：提高优化稳定性和探索效率，避免从零学习视觉特征。图像嵌入与本体感知信息（如末端姿态）拼接输入策略网络。
	- 这使得系统能处理高维视觉输入，而无需大量真实世界数据。
- 奖励函数
	- 用稀疏奖励：仅在任务成功时奖励1，否则0。通过二元分类器实现
	- 训练分类器：收集200正样本（成功）和1000负样本（失败），准确率>95%。对于涉及夹持的任务，添加小负罚以避免不必要操作。
	- 为什么稀疏？避免任务特定奖励整形（reward shaping）的复杂性，与演示结合即可有效。
- 下游机器人系统
	- ?观察表示：相对坐标系，促进空间泛化。每个episode开始随机化末端姿态，动作相对当前帧。
	- **控制器**：
	    - ?接触任务：阻抗控制器（impedance controller），带参考限位，确保安全探索。
	    - ?动态任务：直接命令前馈扭矩（feedforward wrenches），实现开放环加速。
	- ?支持单臂/双臂，多种相机（腕部相机促进自我中心视图，侧相机补全视野）。
- 夹持器控制
	- 对于涉及夹持的任务，使用单独的批评者网络（Critic）处理离散动作（如“开”、“关”、“保持”）。
	- 为什么分离？连续分布近似离散动作困难；使用DQN（Deep Q-Network）训练批评者，稳定学习。
	- 双臂时，动作空间扩展到9种组合（3^2）。推理时，先查询连续动作，再argmax离散动作。
- 人类在环强化学习（Human-in-the-Loop RL）
	- 核心创新：人类监督训练，实时干预纠正错误。
	- 过程：在episode中，如果策略导致不良状态，人类用(SpaceMouse)接管控制，提供校正动作𝑎𝑖𝑡𝑣。
	- 数据处理：校正数据存入演示和RL缓冲区；策略过渡仅存RL缓冲区。
	- 频率：初期频繁干预（演示从各种状态解决问题），后期减少，让机器人自主探索。
	- 优势：**降低样本复杂度，帮助逃离局部最优**。类似于HG-DAgger，但用RL优化而非监督学习。
- 训练过程
	1. **训练奖励分类器**：遥控机器人收集正/负样本（约5分钟），训练二元分类器。
	2. **收集演示**：20-30个成功轨迹，初始化演示缓冲区。重置任务用脚本或手动。
	3. **在线RL训练**：启动Actor和Learner。人类干预直到策略收敛（1-2.5小时）。避免过度长干预，以防价值函数过估计。
	训练动态：成功率和周期时间随时间提升，干预率下降。政策学习反应式（reactive）和预测式（predictive）控制策略。

